{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8031f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this one for evaluating in multi-thread\n",
    "#cd tools/dataset_converter/ && python coco_annotation.py --dataset_path=/home/ehsan/UvA/Accuracy/Keras/Yolov3/Dataset\n",
    "server=1\n",
    "sample=0\n",
    "GPU=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1341224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_dir=\"/home/ehsan/UvA/Accuracy/Keras/YOLOV3/Evaluation/\"\n",
    "dataset_dir=\"/home/ehsan/UvA/Accuracy/Keras/YOLOV3/Dataset/val2017/\"\n",
    "ann='val2017'\n",
    "ann_sample='sample_10_2017'\n",
    "if sample:\n",
    "    ann=ann_sample\n",
    "if server:\n",
    "    _dir=\"/home/ehsan/Accuracy/YOLOV3/Evaluation/\"\n",
    "    #ann=ann+\"_server\"\n",
    "    dataset_dir=\"/home/ehsan/Accuracy/YOLOV3/Dataset/val2017/\"\n",
    "import os\n",
    "_dir=os.path.abspath(os.path.dirname(__file__))\n",
    "print(f'Evalution dir:{_dir}')\n",
    "\n",
    "ann=ann+'.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e59cf17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate mAP for YOLO model on some annotation dataset\n",
    "\"\"\"\n",
    "import os, argparse, time\n",
    "import numpy as np\n",
    "import operator\n",
    "from operator import mul\n",
    "from functools import reduce\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import itertools\n",
    "import collections\n",
    "import threading\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8aea8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-07 15:28:16.678384: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-03-07 15:28:16.726045: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-03-07 15:28:16.726838: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-07 15:28:17.510442: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "import MNN\n",
    "import onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4171a801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolo5.postprocess_np import yolo5_postprocess_np\n",
    "from yolo3.postprocess_np import yolo3_postprocess_np\n",
    "from yolo2.postprocess_np import yolo2_postprocess_np\n",
    "from common.data_utils import preprocess_image\n",
    "from common.utils import get_dataset, get_classes, get_anchors, get_colors, draw_boxes, optimize_tf_gpu, get_custom_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8be84e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "#physical_devices = tf.config.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[1], True)\n",
    "#import tensorflow.lite.gpu.GpuDelegate\n",
    "if GPU:\n",
    "    #tf.debugging.set_log_device_placement(True)\n",
    "    #delegate = GpuDelegate()\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    print(f'GPU list:{physical_devices}')\n",
    "    #tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    #import os\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6182d8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d073c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_tf_gpu(tf, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e8d48c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotation_parse(annotation_lines, class_names):\n",
    "    '''\n",
    "    parse annotation lines to get image dict and ground truth class dict\n",
    "\n",
    "    image dict would be like:\n",
    "    annotation_records = {\n",
    "        '/path/to/000001.jpg': {'100,120,200,235':'dog', '85,63,156,128':'car', ...},\n",
    "        ...\n",
    "    }\n",
    "\n",
    "    ground truth class dict would be like:\n",
    "    classes_records = {\n",
    "        'car': [\n",
    "                ['000001.jpg','100,120,200,235'],\n",
    "                ['000002.jpg','85,63,156,128'],\n",
    "                ...\n",
    "               ],\n",
    "        ...\n",
    "    }\n",
    "    '''\n",
    "    annotation_records = OrderedDict()\n",
    "    classes_records = OrderedDict({class_name: [] for class_name in class_names})\n",
    "\n",
    "    for line in annotation_lines:\n",
    "        box_records = {}\n",
    "        image_name = line.split(' ')[0]\n",
    "        if dataset_dir:\n",
    "            image_name=dataset_dir+image_name.split(\"/\")[-1]\n",
    "        boxes = line.split(' ')[1:]\n",
    "        for box in boxes:\n",
    "            #strip box coordinate and class\n",
    "            class_name = class_names[int(box.split(',')[-1])]\n",
    "            coordinate = ','.join(box.split(',')[:-1])\n",
    "            box_records[coordinate] = class_name\n",
    "            #append or add ground truth class item\n",
    "            record = [os.path.basename(image_name), coordinate]\n",
    "            if class_name in classes_records:\n",
    "                classes_records[class_name].append(record)\n",
    "            else:\n",
    "                classes_records[class_name] = list([record])\n",
    "        annotation_records[image_name] = box_records\n",
    "\n",
    "    return annotation_records, classes_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5aed08b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_gt_record(gt_records, class_names):\n",
    "    '''\n",
    "    Transform the Ground Truth records of a image to prediction format, in\n",
    "    order to show & compare in result pic.\n",
    "\n",
    "    Ground Truth records is a dict with format:\n",
    "        {'100,120,200,235':'dog', '85,63,156,128':'car', ...}\n",
    "\n",
    "    Prediction format:\n",
    "        (boxes, classes, scores)\n",
    "    '''\n",
    "    if gt_records is None or len(gt_records) == 0:\n",
    "        return [], [], []\n",
    "\n",
    "    gt_boxes = []\n",
    "    gt_classes = []\n",
    "    gt_scores = []\n",
    "    for (coordinate, class_name) in gt_records.items():\n",
    "        gt_box = [int(x) for x in coordinate.split(',')]\n",
    "        gt_class = class_names.index(class_name)\n",
    "\n",
    "        gt_boxes.append(gt_box)\n",
    "        gt_classes.append(gt_class)\n",
    "        gt_scores.append(1.0)\n",
    "\n",
    "    return np.array(gt_boxes), np.array(gt_classes), np.array(gt_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "726ab11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_predict_tflite(interpreter, image, anchors, num_classes, conf_threshold, elim_grid_sense, v5_decode):\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # check the type of the input tensor\n",
    "    #if input_details[0]['dtype'] == np.float32:\n",
    "        #floating_model = True\n",
    "\n",
    "    \n",
    "    height = input_details[0]['shape'][1]\n",
    "    width = input_details[0]['shape'][2]\n",
    "    model_input_shape = (height, width)\n",
    "    model_input_shape = (608,608)\n",
    "    #print(f'{input_details}\\nimage shape:{np.array(image).shape}, model input shape {model_input_shape}')\n",
    "    #input()\n",
    "    image_data = preprocess_image(image, model_input_shape)\n",
    "    #origin image shape, in (height, width) format\n",
    "    image_shape = image.size[::-1]\n",
    "\n",
    "    interpreter.set_tensor(input_details[0]['index'], image_data)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    prediction = []\n",
    "    for output_detail in output_details:\n",
    "        output_data = interpreter.get_tensor(output_detail['index'])\n",
    "        prediction.append(output_data)\n",
    "\n",
    "    if len(anchors) == 5:\n",
    "        # YOLOv2 use 5 anchors and have only 1 prediction\n",
    "        assert len(prediction) == 1, 'invalid YOLOv2 prediction number.'\n",
    "        pred_boxes, pred_classes, pred_scores = yolo2_postprocess_np(prediction[0], image_shape, anchors, num_classes, model_input_shape, max_boxes=100, confidence=conf_threshold, elim_grid_sense=elim_grid_sense)\n",
    "    else:\n",
    "        if v5_decode:\n",
    "            pred_boxes, pred_classes, pred_scores = yolo5_postprocess_np(prediction, image_shape, anchors, num_classes, model_input_shape, max_boxes=100, confidence=conf_threshold, elim_grid_sense=True) #enable \"elim_grid_sense\" by default\n",
    "        else:\n",
    "            pred_boxes, pred_classes, pred_scores = yolo3_postprocess_np(prediction, image_shape, anchors, num_classes, model_input_shape, max_boxes=100, confidence=conf_threshold, elim_grid_sense=elim_grid_sense)\n",
    "\n",
    "    return pred_boxes, pred_classes, pred_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8f8e308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_predict_mnn(interpreter, session, image, anchors, num_classes, conf_threshold, elim_grid_sense, v5_decode):\n",
    "    # assume only 1 input tensor for image\n",
    "    input_tensor = interpreter.getSessionInput(session)\n",
    "\n",
    "    # get & resize input shape\n",
    "    input_shape = list(input_tensor.getShape())\n",
    "    if input_shape[0] == 0:\n",
    "        input_shape[0] = 1\n",
    "        interpreter.resizeTensor(input_tensor, tuple(input_shape))\n",
    "        interpreter.resizeSession(session)\n",
    "\n",
    "    # check if input layout is NHWC or NCHW\n",
    "    if input_shape[1] == 3:\n",
    "        #print(\"NCHW input layout\")\n",
    "        batch, channel, height, width = input_shape  #NCHW\n",
    "    elif input_shape[-1] == 3:\n",
    "        #print(\"NHWC input layout\")\n",
    "        batch, height, width, channel = input_shape  #NHWC\n",
    "    else:\n",
    "        # should be MNN.Tensor_DimensionType_Caffe_C4, unsupported now\n",
    "        raise ValueError('unsupported input tensor dimension type')\n",
    "\n",
    "    model_input_shape = (height, width)\n",
    "\n",
    "    # prepare input image\n",
    "    image_data = preprocess_image(image, model_input_shape)\n",
    "    #origin image shape, in (height, width) format\n",
    "    image_shape = image.size[::-1]\n",
    "\n",
    "    # create a temp tensor to copy data\n",
    "    # use TF NHWC layout to align with image data array\n",
    "    # TODO: currently MNN python binding have mem leak when creating MNN.Tensor\n",
    "    # from numpy array, only from tuple is good. So we convert input image to tuple\n",
    "    tmp_input_shape = (batch, height, width, channel)\n",
    "    input_elementsize = reduce(mul, tmp_input_shape)\n",
    "    tmp_input = MNN.Tensor(tmp_input_shape, input_tensor.getDataType(),\\\n",
    "                    tuple(image_data.reshape(input_elementsize, -1)), MNN.Tensor_DimensionType_Tensorflow)\n",
    "\n",
    "    input_tensor.copyFrom(tmp_input)\n",
    "    interpreter.runSession(session)\n",
    "\n",
    "    def get_tensor_list(output_tensors):\n",
    "        # transform the output tensor dict to ordered tensor list, for further postprocess\n",
    "        #\n",
    "        # output tensor list should be like (for YOLOv3):\n",
    "        # [\n",
    "        #  (name, tensor) for (13, 13, 3, num_classes+5),\n",
    "        #  (name, tensor) for (26, 26, 3, num_classes+5),\n",
    "        #  (name, tensor) for (52, 52, 3, num_classes+5)\n",
    "        # ]\n",
    "        output_list = []\n",
    "\n",
    "        for (output_tensor_name, output_tensor) in output_tensors.items():\n",
    "            tensor_shape = output_tensor.getShape()\n",
    "            dim_type = output_tensor.getDimensionType()\n",
    "            tensor_height, tensor_width = tensor_shape[2:4] if dim_type == MNN.Tensor_DimensionType_Caffe else tensor_shape[1:3]\n",
    "\n",
    "            if len(anchors) == 6:\n",
    "                # Tiny YOLOv3\n",
    "                if tensor_height == height//32:\n",
    "                    output_list.insert(0, (output_tensor_name, output_tensor))\n",
    "                elif tensor_height == height//16:\n",
    "                    output_list.insert(1, (output_tensor_name, output_tensor))\n",
    "                else:\n",
    "                    raise ValueError('invalid tensor shape')\n",
    "            elif len(anchors) == 9:\n",
    "                # YOLOv3\n",
    "                if tensor_height == height//32:\n",
    "                    output_list.insert(0, (output_tensor_name, output_tensor))\n",
    "                elif tensor_height == height//16:\n",
    "                    output_list.insert(1, (output_tensor_name, output_tensor))\n",
    "                elif tensor_height == height//8:\n",
    "                    output_list.insert(2, (output_tensor_name, output_tensor))\n",
    "                else:\n",
    "                    raise ValueError('invalid tensor shape')\n",
    "            elif len(anchors) == 5:\n",
    "                # YOLOv2 use 5 anchors and have only 1 prediction\n",
    "                assert len(output_tensors) == 1, 'YOLOv2 model should have only 1 output tensor.'\n",
    "                output_list.insert(0, (output_tensor_name, output_tensor))\n",
    "            else:\n",
    "                raise ValueError('invalid anchor number')\n",
    "\n",
    "        return output_list\n",
    "\n",
    "    output_tensors = interpreter.getSessionOutputAll(session)\n",
    "    output_tensor_list = get_tensor_list(output_tensors)\n",
    "\n",
    "    prediction = []\n",
    "    for (output_tensor_name, output_tensor) in output_tensor_list:\n",
    "        output_shape = output_tensor.getShape()\n",
    "        output_elementsize = reduce(mul, output_shape)\n",
    "\n",
    "        # check output channel to confirm if output layout\n",
    "        # is NHWC or NCHW\n",
    "        if len(anchors) == 5:\n",
    "            out_channel = (num_classes + 5) * 5\n",
    "        else:\n",
    "            out_channel = (num_classes + 5) * (len(anchors)//3)\n",
    "\n",
    "        #if output_shape[1] == out_channel:\n",
    "            #print(\"NCHW output layout\")\n",
    "        #elif output_shape[-1] == out_channel:\n",
    "            #print(\"NHWC output layout\")\n",
    "        #else:\n",
    "            #raise ValueError('invalid output layout or shape')\n",
    "\n",
    "        assert output_tensor.getDataType() == MNN.Halide_Type_Float\n",
    "\n",
    "        # copy output tensor to host, for further postprocess\n",
    "        tmp_output = MNN.Tensor(output_shape, output_tensor.getDataType(),\\\n",
    "                    #np.zeros(output_shape, dtype=float), output_tensor.getDimensionType())\n",
    "                    tuple(np.zeros(output_shape, dtype=float).reshape(output_elementsize, -1)), output_tensor.getDimensionType())\n",
    "\n",
    "        output_tensor.copyToHostTensor(tmp_output)\n",
    "        #tmp_output.printTensorData()\n",
    "\n",
    "        output_data = np.array(tmp_output.getData(), dtype=float).reshape(output_shape)\n",
    "        # our postprocess code based on TF NHWC layout, so if the output format\n",
    "        # doesn't match, we need to transpose\n",
    "        if output_tensor.getDimensionType() == MNN.Tensor_DimensionType_Caffe and output_shape[1] == out_channel: # double check if it's NCHW format\n",
    "            output_data = output_data.transpose((0,2,3,1))\n",
    "        elif output_tensor.getDimensionType() == MNN.Tensor_DimensionType_Caffe_C4:\n",
    "            raise ValueError('unsupported output tensor dimension type')\n",
    "\n",
    "        prediction.append(output_data)\n",
    "\n",
    "    if len(anchors) == 5:\n",
    "        # YOLOv2 use 5 anchors and have only 1 prediction\n",
    "        assert len(prediction) == 1, 'invalid YOLOv2 prediction number.'\n",
    "        pred_boxes, pred_classes, pred_scores = yolo2_postprocess_np(prediction[0], image_shape, anchors, num_classes, model_input_shape, max_boxes=100, confidence=conf_threshold, elim_grid_sense=elim_grid_sense)\n",
    "    else:\n",
    "        if v5_decode:\n",
    "            pred_boxes, pred_classes, pred_scores = yolo5_postprocess_np(prediction, image_shape, anchors, num_classes, model_input_shape, max_boxes=100, confidence=conf_threshold, elim_grid_sense=True) #enable \"elim_grid_sense\" by default\n",
    "        else:\n",
    "            pred_boxes, pred_classes, pred_scores = yolo3_postprocess_np(prediction, image_shape, anchors, num_classes, model_input_shape, max_boxes=100, confidence=conf_threshold, elim_grid_sense=elim_grid_sense)\n",
    "\n",
    "    return pred_boxes, pred_classes, pred_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c15b76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_predict_pb(model, image, anchors, num_classes, model_input_shape, conf_threshold, elim_grid_sense, v5_decode):\n",
    "    # NOTE: TF 1.x frozen pb graph need to specify input/output tensor name\n",
    "    # so we hardcode the input/output tensor names here to get them from model\n",
    "    if len(anchors) == 6:\n",
    "        output_tensor_names = ['graph/predict_conv_1/BiasAdd:0', 'graph/predict_conv_2/BiasAdd:0']\n",
    "    elif len(anchors) == 9:\n",
    "        output_tensor_names = ['graph/predict_conv_1/BiasAdd:0', 'graph/predict_conv_2/BiasAdd:0', 'graph/predict_conv_3/BiasAdd:0']\n",
    "    elif len(anchors) == 5:\n",
    "        # YOLOv2 use 5 anchors and have only 1 prediction\n",
    "        output_tensor_names = ['graph/predict_conv/BiasAdd:0']\n",
    "    else:\n",
    "        raise ValueError('invalid anchor number')\n",
    "\n",
    "    # assume only 1 input tensor for image\n",
    "    input_tensor_name = 'graph/image_input:0'\n",
    "\n",
    "    # get input/output tensors\n",
    "    image_input = model.get_tensor_by_name(input_tensor_name)\n",
    "    output_tensors = [model.get_tensor_by_name(output_tensor_name) for output_tensor_name in output_tensor_names]\n",
    "\n",
    "    batch, height, width, channel = image_input.shape\n",
    "    model_input_shape = (int(height), int(width))\n",
    "\n",
    "    # prepare input image\n",
    "    image_data = preprocess_image(image, model_input_shape)\n",
    "    #origin image shape, in (height, width) format\n",
    "    image_shape = image.size[::-1]\n",
    "\n",
    "    with tf.Session(graph=model) as sess:\n",
    "        prediction = sess.run(output_tensors, feed_dict={\n",
    "            image_input: image_data\n",
    "        })\n",
    "\n",
    "    if len(anchors) == 5:\n",
    "        # YOLOv2 use 5 anchors and have only 1 prediction\n",
    "        assert len(prediction) == 1, 'invalid YOLOv2 prediction number.'\n",
    "        pred_boxes, pred_classes, pred_scores = yolo2_postprocess_np(prediction[0], image_shape, anchors, num_classes, model_input_shape, max_boxes=100, confidence=conf_threshold, elim_grid_sense=elim_grid_sense)\n",
    "    else:\n",
    "        if v5_decode:\n",
    "            pred_boxes, pred_classes, pred_scores = yolo5_postprocess_np(prediction, image_shape, anchors, num_classes, model_input_shape, max_boxes=100, confidence=conf_threshold, elim_grid_sense=True) #enable \"elim_grid_sense\" by default\n",
    "        else:\n",
    "            pred_boxes, pred_classes, pred_scores = yolo3_postprocess_np(prediction, image_shape, anchors, num_classes, model_input_shape, max_boxes=100, confidence=conf_threshold, elim_grid_sense=elim_grid_sense)\n",
    "\n",
    "    return pred_boxes, pred_classes, pred_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2125d5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_predict_onnx(model, image, anchors, num_classes, conf_threshold, elim_grid_sense, v5_decode):\n",
    "    input_tensors = []\n",
    "    for i, input_tensor in enumerate(model.get_inputs()):\n",
    "        input_tensors.append(input_tensor)\n",
    "\n",
    "    # assume only 1 input tensor for image\n",
    "    assert len(input_tensors) == 1, 'invalid input tensor number.'\n",
    "\n",
    "    # check if input layout is NHWC or NCHW\n",
    "    if input_tensors[0].shape[1] == 3:\n",
    "        batch, channel, height, width = input_tensors[0].shape  #NCHW\n",
    "    else:\n",
    "        batch, height, width, channel = input_tensors[0].shape  #NHWC\n",
    "\n",
    "    model_input_shape = (height, width)\n",
    "\n",
    "    output_tensors = []\n",
    "    for i, output_tensor in enumerate(model.get_outputs()):\n",
    "        output_tensors.append(output_tensor)\n",
    "\n",
    "    # check output channel to confirm if output layout\n",
    "    # is NHWC or NCHW\n",
    "    if len(anchors) == 5:\n",
    "        out_channel = (num_classes + 5) * 5\n",
    "    else:\n",
    "        out_channel = (num_classes + 5) * (len(anchors)//3)\n",
    "\n",
    "    if output_tensors[0].shape[1] == out_channel:\n",
    "        #print(\"NCHW output layout\")\n",
    "        pass\n",
    "    elif output_tensors[0].shape[-1] == out_channel:\n",
    "        #print(\"NHWC output layout\")\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError('invalid output layout or shape')\n",
    "\n",
    "\n",
    "    # prepare input image\n",
    "    image_data = preprocess_image(image, model_input_shape)\n",
    "    #origin image shape, in (height, width) format\n",
    "    image_shape = image.size[::-1]\n",
    "\n",
    "    if input_tensors[0].shape[1] == 3:\n",
    "        # transpose image for NCHW layout\n",
    "        image_data = image_data.transpose((0,3,1,2))\n",
    "\n",
    "    feed = {input_tensors[0].name: image_data}\n",
    "    prediction = model.run(None, feed)\n",
    "\n",
    "    if output_tensors[0].shape[1] == out_channel:\n",
    "        # transpose prediction array for NCHW layout\n",
    "        prediction = [p.transpose((0,2,3,1)) for p in prediction]\n",
    "\n",
    "    if len(anchors) == 5:\n",
    "        # YOLOv2 use 5 anchors and have only 1 prediction\n",
    "        assert len(prediction) == 1, 'invalid YOLOv2 prediction number.'\n",
    "        pred_boxes, pred_classes, pred_scores = yolo2_postprocess_np(prediction[0], image_shape, anchors, num_classes, model_input_shape, max_boxes=100, confidence=conf_threshold, elim_grid_sense=elim_grid_sense)\n",
    "    else:\n",
    "        if v5_decode:\n",
    "            pred_boxes, pred_classes, pred_scores = yolo5_postprocess_np(prediction, image_shape, anchors, num_classes, model_input_shape, max_boxes=100, confidence=conf_threshold, elim_grid_sense=True) #enable \"elim_grid_sense\" by default\n",
    "        else:\n",
    "            pred_boxes, pred_classes, pred_scores = yolo3_postprocess_np(prediction, image_shape, anchors, num_classes, model_input_shape, max_boxes=100, confidence=conf_threshold, elim_grid_sense=elim_grid_sense)\n",
    "\n",
    "    return pred_boxes, pred_classes, pred_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91b16a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_predict_keras(model, image, anchors, num_classes, model_input_shape, conf_threshold, elim_grid_sense, v5_decode):\n",
    "    image_data = preprocess_image(image, model_input_shape)\n",
    "    #origin image shape, in (height, width) format\n",
    "    image_shape = image.size[::-1]\n",
    "\n",
    "    prediction = model.predict([image_data])\n",
    "    if type(prediction) is not list:\n",
    "        prediction = [prediction]\n",
    "\n",
    "    if len(anchors) == 5:\n",
    "        # YOLOv2 use 5 anchors\n",
    "        pred_boxes, pred_classes, pred_scores = yolo2_postprocess_np(prediction[0], image_shape, anchors, num_classes, model_input_shape, max_boxes=100, confidence=conf_threshold, elim_grid_sense=elim_grid_sense)\n",
    "    else:\n",
    "        if v5_decode:\n",
    "            pred_boxes, pred_classes, pred_scores = yolo5_postprocess_np(prediction, image_shape, anchors, num_classes, model_input_shape, max_boxes=100, confidence=conf_threshold, elim_grid_sense=True) #enable \"elim_grid_sense\" by default\n",
    "        else:\n",
    "            pred_boxes, pred_classes, pred_scores = yolo3_postprocess_np(prediction, image_shape, anchors, num_classes, model_input_shape, max_boxes=100, confidence=conf_threshold, elim_grid_sense=elim_grid_sense)\n",
    "\n",
    "    return pred_boxes, pred_classes, pred_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aebc2056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_class_records(model, model_format, annotation_records, anchors, class_names, model_input_shape, conf_threshold, elim_grid_sense, v5_decode, save_result,indx=-1):\n",
    "    '''\n",
    "    Do the predict with YOLO model on annotation images to get predict class dict\n",
    "\n",
    "    predict class dict would contain image_name, coordinary and score, and\n",
    "    sorted by score:\n",
    "    pred_classes_records = {\n",
    "        'car': [\n",
    "                ['000001.jpg','94,115,203,232',0.98],\n",
    "                ['000002.jpg','82,64,154,128',0.93],\n",
    "                ...\n",
    "               ],\n",
    "        ...\n",
    "    }\n",
    "    '''\n",
    "    if model_format == 'MNN':\n",
    "        #MNN inference engine need create session\n",
    "        session = model.createSession()\n",
    "\n",
    "    # create txt file to save prediction result, with\n",
    "    # save format as annotation file but adding score, like:\n",
    "    #\n",
    "    # path/to/img1.jpg 50,100,150,200,0,0.86 30,50,200,120,3,0.95\n",
    "    #\n",
    "    os.makedirs(_dir+'/result', exist_ok=True)\n",
    "    result_file = open(os.path.join(_dir,'result/detection_result.txt'), 'w')\n",
    "\n",
    "    pred_classes_records = OrderedDict()\n",
    "    \n",
    "    '''item_slice = itertools.islice(annotation_records.items(), 2000, 3000)\n",
    "    item_slice=collections.OrderedDict(item_slice)\n",
    "    num_items = sum(1 for _ in item_slice.items())\n",
    "    print(num_items)\n",
    "    for k,i in item_slice.items():\n",
    "        print(k,i)\n",
    "    pbar = tqdm(total=len(item_slice), desc='Eval model')'''\n",
    "    if indx==0:\n",
    "        pbar = tqdm(total=len(annotation_records), desc=f'Eval model thread {indx}')\n",
    "    for (image_name, gt_records) in annotation_records.items():\n",
    "        image = Image.open(image_name)\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        image_array = np.array(image, dtype='uint8')\n",
    "\n",
    "        # support of tflite model\n",
    "        if model_format == 'TFLITE':\n",
    "            start=time.time()\n",
    "            pred_boxes, pred_classes, pred_scores = yolo_predict_tflite(model, image, anchors, len(class_names), conf_threshold, elim_grid_sense, v5_decode)\n",
    "            end = time.time()\n",
    "            if indx==0:\n",
    "                print(\"inference time cost: {:.6f}s\".format(end - start))\n",
    "        # support of MNN model\n",
    "        elif model_format == 'MNN':\n",
    "            pred_boxes, pred_classes, pred_scores = yolo_predict_mnn(model, session, image, anchors, len(class_names), conf_threshold, elim_grid_sense, v5_decode)\n",
    "        # support of TF 1.x frozen pb model\n",
    "        elif model_format == 'PB':\n",
    "            pred_boxes, pred_classes, pred_scores = yolo_predict_pb(model, image, anchors, len(class_names), model_input_shape, conf_threshold, elim_grid_sense, v5_decode)\n",
    "        # support of ONNX model\n",
    "        elif model_format == 'ONNX':\n",
    "            pred_boxes, pred_classes, pred_scores = yolo_predict_onnx(model, image, anchors, len(class_names), conf_threshold, elim_grid_sense, v5_decode)\n",
    "        # normal keras h5 model\n",
    "        elif model_format == 'H5':\n",
    "            pred_boxes, pred_classes, pred_scores = yolo_predict_keras(model, image, anchors, len(class_names), model_input_shape, conf_threshold, elim_grid_sense, v5_decode)\n",
    "        else:\n",
    "            raise ValueError('invalid model format')\n",
    "\n",
    "        #print('Found {} boxes for {}'.format(len(pred_boxes), image_name))\n",
    "        image.close()\n",
    "        if indx==0:\n",
    "            pbar.update(1)\n",
    "\n",
    "        # save prediction result to txt\n",
    "        result_file.write(image_name)\n",
    "        for box, cls, score in zip(pred_boxes, pred_classes, pred_scores):\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            box_annotation = \" %d,%d,%d,%d,%d,%f\" % (\n",
    "                xmin, ymin, xmax, ymax, cls, score)\n",
    "            result_file.write(box_annotation)\n",
    "        result_file.write('\\n')\n",
    "        result_file.flush()\n",
    "\n",
    "        if save_result:\n",
    "\n",
    "            gt_boxes, gt_classes, gt_scores = transform_gt_record(gt_records, class_names)\n",
    "\n",
    "            result_dir=os.path.join('result','detection')\n",
    "            os.makedirs(result_dir, exist_ok=True)\n",
    "            colors = get_colors(len(class_names))\n",
    "            image_array = draw_boxes(image_array, gt_boxes, gt_classes, gt_scores, class_names, colors=None, show_score=False)\n",
    "            image_array = draw_boxes(image_array, pred_boxes, pred_classes, pred_scores, class_names, colors)\n",
    "            image = Image.fromarray(image_array)\n",
    "            # here we handle the RGBA image\n",
    "            if(len(image.split()) == 4):\n",
    "                r, g, b, a = image.split()\n",
    "                image = Image.merge(\"RGB\", (r, g, b))\n",
    "            image.save(os.path.join(result_dir, image_name.split(os.path.sep)[-1]))\n",
    "\n",
    "        # Nothing detected\n",
    "        if pred_boxes is None or len(pred_boxes) == 0:\n",
    "            continue\n",
    "\n",
    "        for box, cls, score in zip(pred_boxes, pred_classes, pred_scores):\n",
    "            pred_class_name = class_names[cls]\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            coordinate = \"{},{},{},{}\".format(xmin, ymin, xmax, ymax)\n",
    "\n",
    "            #append or add predict class item\n",
    "            record = [os.path.basename(image_name), coordinate, score]\n",
    "            if pred_class_name in pred_classes_records:\n",
    "                pred_classes_records[pred_class_name].append(record)\n",
    "            else:\n",
    "                pred_classes_records[pred_class_name] = list([record])\n",
    "\n",
    "    # sort pred_classes_records for each class according to score\n",
    "    start = time.time()\n",
    "    for pred_class_list in pred_classes_records.values():\n",
    "        pred_class_list.sort(key=lambda ele: ele[2], reverse=True)\n",
    "    end = time.time()\n",
    "    print(\"post time cost: {:.6f}s\".format(end - start))\n",
    "    if indx==0:\n",
    "        pbar.close()\n",
    "    result_file.close()\n",
    "    return pred_classes_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cfeff4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef process_items(items):\\n    for key, value in items:\\n        print(key,value)\\n        print(\"key\")\\n\\ndata = collections.OrderedDict([(\\'a\\', 1), (\\'b\\', 2), (\\'c\\', 3), (\\'d\\', 4), (\\'e\\', 5)])\\n\\nnum_threads = 2\\nchunk_size = len(data) // num_threads\\n\\nthreads = []\\nfor i in range(num_threads):\\n    start = i * chunk_size\\n    end = start + chunk_size\\n    if i == num_threads - 1:\\n        end = len(data)\\n    item_slice = itertools.islice(data.items(), start, end)\\n    item_slice=collections.OrderedDict(item_slice)\\n    print(item_slice)\\n    for key, value in item_slice.items():\\n        print(key,value)\\n    thread = threading.Thread(target=process_items, args=(item_slice,))\\n    threads.append(thread)\\n    thread.start()\\n\\nfor thread in threads:\\n    thread.join()\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "def process_items(items):\n",
    "    for key, value in items:\n",
    "        print(key,value)\n",
    "        print(\"key\")\n",
    "\n",
    "data = collections.OrderedDict([('a', 1), ('b', 2), ('c', 3), ('d', 4), ('e', 5)])\n",
    "\n",
    "num_threads = 2\n",
    "chunk_size = len(data) // num_threads\n",
    "\n",
    "threads = []\n",
    "for i in range(num_threads):\n",
    "    start = i * chunk_size\n",
    "    end = start + chunk_size\n",
    "    if i == num_threads - 1:\n",
    "        end = len(data)\n",
    "    item_slice = itertools.islice(data.items(), start, end)\n",
    "    item_slice=collections.OrderedDict(item_slice)\n",
    "    print(item_slice)\n",
    "    for key, value in item_slice.items():\n",
    "        print(key,value)\n",
    "    thread = threading.Thread(target=process_items, args=(item_slice,))\n",
    "    threads.append(thread)\n",
    "    thread.start()\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0e9a797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_iou(pred_box, gt_box):\n",
    "    '''\n",
    "    Calculate iou for predict box and ground truth box\n",
    "    Param\n",
    "         pred_box: predict box coordinate\n",
    "                   (xmin,ymin,xmax,ymax) format\n",
    "         gt_box: ground truth box coordinate\n",
    "                 (xmin,ymin,xmax,ymax) format\n",
    "    Return\n",
    "         iou value\n",
    "    '''\n",
    "    # get intersection box\n",
    "    inter_box = [max(pred_box[0], gt_box[0]), max(pred_box[1], gt_box[1]), min(pred_box[2], gt_box[2]), min(pred_box[3], gt_box[3])]\n",
    "    inter_w = max(0.0, inter_box[2] - inter_box[0] + 1)\n",
    "    inter_h = max(0.0, inter_box[3] - inter_box[1] + 1)\n",
    "\n",
    "    # compute overlap (IoU) = area of intersection / area of union\n",
    "    pred_area = (pred_box[2] - pred_box[0] + 1) * (pred_box[3] - pred_box[1] + 1)\n",
    "    gt_area = (gt_box[2] - gt_box[0] + 1) * (gt_box[3] - gt_box[1] + 1)\n",
    "    inter_area = inter_w * inter_h\n",
    "    union_area = pred_area + gt_area - inter_area\n",
    "    return 0 if union_area == 0 else float(inter_area) / float(union_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b35b047b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def match_gt_box(pred_record, gt_records, iou_threshold=0.5):\n",
    "    '''\n",
    "    Search gt_records list and try to find a matching box for the predict box\n",
    "\n",
    "    Param\n",
    "         pred_record: with format ['image_file', 'xmin,ymin,xmax,ymax', score]\n",
    "         gt_records: record list with format\n",
    "                     [\n",
    "                      ['image_file', 'xmin,ymin,xmax,ymax', 'usage'],\n",
    "                      ['image_file', 'xmin,ymin,xmax,ymax', 'usage'],\n",
    "                      ...\n",
    "                     ]\n",
    "         iou_threshold:\n",
    "\n",
    "         pred_record and gt_records should be from same annotation image file\n",
    "\n",
    "    Return\n",
    "         matching gt_record index. -1 when there's no matching gt\n",
    "    '''\n",
    "    max_iou = 0.0\n",
    "    max_index = -1\n",
    "    #get predict box coordinate\n",
    "    pred_box = [float(x) for x in pred_record[1].split(',')]\n",
    "\n",
    "    for i, gt_record in enumerate(gt_records):\n",
    "        #get ground truth box coordinate\n",
    "        gt_box = [float(x) for x in gt_record[1].split(',')]\n",
    "        iou = box_iou(pred_box, gt_box)\n",
    "\n",
    "        # if the ground truth has been assigned to other\n",
    "        # prediction, we couldn't reuse it\n",
    "        if iou > max_iou and gt_record[2] == 'unused' and pred_record[0] == gt_record[0]:\n",
    "            max_iou = iou\n",
    "            max_index = i\n",
    "\n",
    "    # drop the prediction if couldn't match iou threshold\n",
    "    if max_iou < iou_threshold:\n",
    "        max_index = -1\n",
    "\n",
    "    return max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bf322c6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def voc_ap(rec, prec):\n",
    "    \"\"\"\n",
    "    --- Official matlab code VOC2012---\n",
    "    mrec=[0 ; rec ; 1];\n",
    "    mpre=[0 ; prec ; 0];\n",
    "    for i=numel(mpre)-1:-1:1\n",
    "        mpre(i)=max(mpre(i),mpre(i+1));\n",
    "    end\n",
    "    i=find(mrec(2:end)~=mrec(1:end-1))+1;\n",
    "    ap=sum((mrec(i)-mrec(i-1)).*mpre(i));\n",
    "    \"\"\"\n",
    "    rec.insert(0, 0.0)  # insert 0.0 at begining of list\n",
    "    rec.append(1.0)  # insert 1.0 at end of list\n",
    "    mrec = rec[:]\n",
    "    prec.insert(0, 0.0)  # insert 0.0 at begining of list\n",
    "    prec.append(0.0)  # insert 0.0 at end of list\n",
    "    mpre = prec[:]\n",
    "    \"\"\"\n",
    "     This part makes the precision monotonically decreasing\n",
    "      (goes from the end to the beginning)\n",
    "    \"\"\"\n",
    "    # matlab indexes start in 1 but python in 0, so I have to do:\n",
    "    #   range(start=(len(mpre) - 2), end=0, step=-1)\n",
    "    # also the python function range excludes the end, resulting in:\n",
    "    #   range(start=(len(mpre) - 2), end=-1, step=-1)\n",
    "    for i in range(len(mpre) - 2, -1, -1):\n",
    "        mpre[i] = max(mpre[i], mpre[i + 1])\n",
    "    \"\"\"\n",
    "     This part creates a list of indexes where the recall changes\n",
    "    \"\"\"\n",
    "    # matlab: i=find(mrec(2:end)~=mrec(1:end-1))+1;\n",
    "    i_list = []\n",
    "    for i in range(1, len(mrec)):\n",
    "        if mrec[i] != mrec[i - 1]:\n",
    "            i_list.append(i)  # if it was matlab would be i + 1\n",
    "    \"\"\"\n",
    "     The Average Precision (AP) is the area under the curve\n",
    "      (numerical integration)\n",
    "    \"\"\"\n",
    "    # matlab: ap=sum((mrec(i)-mrec(i-1)).*mpre(i));\n",
    "    ap = 0.0\n",
    "    for i in i_list:\n",
    "        ap += ((mrec[i] - mrec[i - 1]) * mpre[i])\n",
    "    return ap, mrec, mpre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "122ff7a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef voc_ap(rec, prec, use_07_metric=False):\\n    if use_07_metric:\\n        # 11 point metric\\n        ap = 0.\\n        for t in np.arange(0., 1.1, 0.1):\\n            if np.sum(rec >= t) == 0:\\n                p = 0\\n            else:\\n                p = np.max(prec[rec >= t])\\n            ap = ap + p / 11.\\n    else:\\n        mrec = np.concatenate(([0.], rec, [1.]))\\n        mpre = np.concatenate(([0.], prec, [0.]))\\n        for i in range(mpre.size - 1, 0, -1):\\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\\n        i = np.where(mrec[1:] != mrec[:-1])[0]\\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\\n    return ap, mrec, mpre\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def voc_ap(rec, prec, use_07_metric=False):\n",
    "    if use_07_metric:\n",
    "        # 11 point metric\n",
    "        ap = 0.\n",
    "        for t in np.arange(0., 1.1, 0.1):\n",
    "            if np.sum(rec >= t) == 0:\n",
    "                p = 0\n",
    "            else:\n",
    "                p = np.max(prec[rec >= t])\n",
    "            ap = ap + p / 11.\n",
    "    else:\n",
    "        mrec = np.concatenate(([0.], rec, [1.]))\n",
    "        mpre = np.concatenate(([0.], prec, [0.]))\n",
    "        for i in range(mpre.size - 1, 0, -1):\n",
    "            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "        i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap, mrec, mpre\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1683423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rec_prec(true_positive, false_positive, gt_records):\n",
    "    '''\n",
    "    Calculate precision/recall based on true_positive, false_positive\n",
    "    result.\n",
    "    '''\n",
    "    cumsum = 0\n",
    "    for idx, val in enumerate(false_positive):\n",
    "        false_positive[idx] += cumsum\n",
    "        cumsum += val\n",
    "\n",
    "    cumsum = 0\n",
    "    for idx, val in enumerate(true_positive):\n",
    "        true_positive[idx] += cumsum\n",
    "        cumsum += val\n",
    "\n",
    "    rec = true_positive[:]\n",
    "    for idx, val in enumerate(true_positive):\n",
    "        rec[idx] = (float(true_positive[idx]) / len(gt_records)) if len(gt_records) != 0 else 0\n",
    "\n",
    "    prec = true_positive[:]\n",
    "    for idx, val in enumerate(true_positive):\n",
    "        prec[idx] = float(true_positive[idx]) / (false_positive[idx] + true_positive[idx])\n",
    "\n",
    "    return rec, prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3c59a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_rec_prec(rec, prec, mrec, mprec, class_name, ap):\n",
    "    \"\"\"\n",
    "     Draw plot\n",
    "    \"\"\"\n",
    "    plt.plot(rec, prec, '-o')\n",
    "    # add a new penultimate point to the list (mrec[-2], 0.0)\n",
    "    # since the last line segment (and respective area) do not affect the AP value\n",
    "    area_under_curve_x = mrec[:-1] + [mrec[-2]] + [mrec[-1]]\n",
    "    area_under_curve_y = mprec[:-1] + [0.0] + [mprec[-1]]\n",
    "    plt.fill_between(area_under_curve_x, 0, area_under_curve_y, alpha=0.2, edgecolor='r')\n",
    "    # set window title\n",
    "    fig = plt.gcf() # gcf - get current figure\n",
    "    # Ehsan: I used the except part because newer version of plt library is different (the first one does not work for newer versions)\n",
    "    try:\n",
    "        fig.canvas.set_window_title('AP ' + class_name)\n",
    "    except:\n",
    "        plt.get_current_fig_manager().set_window_title('AP ' + class_name)\n",
    "    # set plot title\n",
    "    plt.title('class: ' + class_name + ' AP = {}%'.format(ap*100))\n",
    "    #plt.suptitle('This is a somewhat long figure title', fontsize=16)\n",
    "    # set axis titles\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    # optional - set axes\n",
    "    axes = plt.gca() # gca - get current axes\n",
    "    axes.set_xlim([0.0,1.0])\n",
    "    axes.set_ylim([0.0,1.05]) # .05 to give some extra space\n",
    "    # Alternative option -> wait for button to be pressed\n",
    "    #while not plt.waitforbuttonpress(): pass # wait for key display\n",
    "    # Alternative option -> normal display\n",
    "    #plt.show()\n",
    "    # save the plot\n",
    "    rec_prec_plot_path = os.path.join(_dir+'/result/classes')\n",
    "    os.makedirs(rec_prec_plot_path, exist_ok=True)\n",
    "    fig.savefig(os.path.join(rec_prec_plot_path, class_name + \".png\"))\n",
    "    plt.cla() # clear axes for next plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb974e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh\n",
    "import bokeh.io as bokeh_io\n",
    "import bokeh.plotting as bokeh_plotting\n",
    "def generate_rec_prec_html(mrec, mprec, scores, class_name, ap):\n",
    "    \"\"\"\n",
    "     generate dynamic P-R curve HTML page for each class\n",
    "    \"\"\"\n",
    "    # bypass invalid class\n",
    "    if len(mrec) == 0 or len(mprec) == 0 or len(scores) == 0:\n",
    "        return\n",
    "\n",
    "    rec_prec_plot_path = os.path.join(_dir+'/result/classes')\n",
    "    os.makedirs(rec_prec_plot_path, exist_ok=True)\n",
    "    bokeh_io.output_file(os.path.join(rec_prec_plot_path, class_name + '.html'), title='P-R curve for ' + class_name)\n",
    "\n",
    "    # prepare curve data\n",
    "    area_under_curve_x = mrec[:-1] + [mrec[-2]] + [mrec[-1]]\n",
    "    area_under_curve_y = mprec[:-1] + [0.0] + [mprec[-1]]\n",
    "    score_on_curve = [0.0] + scores[:-1] + [0.0] + [scores[-1]] + [1.0]\n",
    "    source = bokeh.models.ColumnDataSource(data={\n",
    "      'rec'      : area_under_curve_x,\n",
    "      'prec' : area_under_curve_y,\n",
    "      'score' : score_on_curve,\n",
    "    })\n",
    "\n",
    "    # Ehsan: try except for making compatibility for newer versions of libraries\n",
    "    # prepare plot figure\n",
    "    plt_title = 'class: ' + class_name + ' AP = {}%'.format(ap*100)\n",
    "    try:\n",
    "        plt = bokeh_plotting.figure(plot_height=200 ,plot_width=200, tools=\"\", toolbar_location=None,\n",
    "               title=plt_title, sizing_mode=\"scale_width\")\n",
    "    except:\n",
    "        plt = bokeh_plotting.figure(height=200 ,width=200, tools=\"\", toolbar_location=None,\n",
    "               title=plt_title, sizing_mode=\"scale_width\")\n",
    "    plt.background_fill_color = \"#f5f5f5\"\n",
    "    plt.grid.grid_line_color = \"white\"\n",
    "    plt.xaxis.axis_label = 'Recall'\n",
    "    plt.yaxis.axis_label = 'Precision'\n",
    "    plt.axis.axis_line_color = None\n",
    "\n",
    "    # draw curve data\n",
    "    plt.line(x='rec', y='prec', line_width=2, color='#ebbd5b', source=source)\n",
    "    plt.add_tools(bokeh.models.HoverTool(\n",
    "      tooltips=[\n",
    "        ( 'score', '@score{0.0000 a}'),\n",
    "        ( 'Prec', '@prec'),\n",
    "        ( 'Recall', '@rec'),\n",
    "      ],\n",
    "      formatters={\n",
    "        'rec'      : 'printf',\n",
    "        'prec' : 'printf',\n",
    "      },\n",
    "      mode='vline'\n",
    "    ))\n",
    "    bokeh_io.save(plt)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1368aeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_axes(r, t, fig, axes):\n",
    "    \"\"\"\n",
    "     Plot - adjust axes\n",
    "    \"\"\"\n",
    "    # get text width for re-scaling\n",
    "    bb = t.get_window_extent(renderer=r)\n",
    "    text_width_inches = bb.width / fig.dpi\n",
    "    # get axis width in inches\n",
    "    current_fig_width = fig.get_figwidth()\n",
    "    new_fig_width = current_fig_width + text_width_inches\n",
    "    propotion = new_fig_width / current_fig_width\n",
    "    # get axis limit\n",
    "    x_lim = axes.get_xlim()\n",
    "    axes.set_xlim([x_lim[0], x_lim[1]*propotion])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef2f68fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_plot_func(dictionary, n_classes, window_title, plot_title, x_label, output_path, to_show, plot_color, true_p_bar):\n",
    "    \"\"\"\n",
    "     Draw plot using Matplotlib\n",
    "    \"\"\"\n",
    "    # sort the dictionary by decreasing value, into a list of tuples\n",
    "    sorted_dic_by_value = sorted(dictionary.items(), key=operator.itemgetter(1))\n",
    "    # unpacking the list of tuples into two lists\n",
    "    sorted_keys, sorted_values = zip(*sorted_dic_by_value)\n",
    "    #\n",
    "    if true_p_bar != \"\":\n",
    "        \"\"\"\n",
    "         Special case to draw in (green=true predictions) & (red=false predictions)\n",
    "        \"\"\"\n",
    "        fp_sorted = []\n",
    "        tp_sorted = []\n",
    "        for key in sorted_keys:\n",
    "            fp_sorted.append(dictionary[key] - true_p_bar[key])\n",
    "            tp_sorted.append(true_p_bar[key])\n",
    "        plt.barh(range(n_classes), fp_sorted, align='center', color='crimson', label='False Predictions')\n",
    "        plt.barh(range(n_classes), tp_sorted, align='center', color='forestgreen', label='True Predictions', left=fp_sorted)\n",
    "        # add legend\n",
    "        plt.legend(loc='lower right')\n",
    "        \"\"\"\n",
    "         Write number on side of bar\n",
    "        \"\"\"\n",
    "        fig = plt.gcf() # gcf - get current figure\n",
    "        axes = plt.gca()\n",
    "        r = fig.canvas.get_renderer()\n",
    "        for i, val in enumerate(sorted_values):\n",
    "            fp_val = fp_sorted[i]\n",
    "            tp_val = tp_sorted[i]\n",
    "            fp_str_val = \" \" + str(fp_val)\n",
    "            tp_str_val = fp_str_val + \" \" + str(tp_val)\n",
    "            # trick to paint multicolor with offset:\n",
    "            #   first paint everything and then repaint the first number\n",
    "            t = plt.text(val, i, tp_str_val, color='forestgreen', va='center', fontweight='bold')\n",
    "            plt.text(val, i, fp_str_val, color='crimson', va='center', fontweight='bold')\n",
    "            if i == (len(sorted_values)-1): # largest bar\n",
    "                adjust_axes(r, t, fig, axes)\n",
    "    else:\n",
    "      plt.barh(range(n_classes), sorted_values, color=plot_color)\n",
    "      \"\"\"\n",
    "       Write number on side of bar\n",
    "      \"\"\"\n",
    "      fig = plt.gcf() # gcf - get current figure\n",
    "      axes = plt.gca()\n",
    "      r = fig.canvas.get_renderer()\n",
    "      for i, val in enumerate(sorted_values):\n",
    "          str_val = \" \" + str(val) # add a space before\n",
    "          if val < 1.0:\n",
    "              str_val = \" {0:.2f}\".format(val)\n",
    "          t = plt.text(val, i, str_val, color=plot_color, va='center', fontweight='bold')\n",
    "          # re-set axes to show number inside the figure\n",
    "          if i == (len(sorted_values)-1): # largest bar\n",
    "              adjust_axes(r, t, fig, axes)\n",
    "    # set window title\n",
    "    \n",
    "    # Ehsan: I used the except part because newer version of plt library is different (the first one does not work for newer versions)\n",
    "    try:\n",
    "        fig.canvas.set_window_title(window_title)\n",
    "    except:\n",
    "        plt.get_current_fig_manager().set_window_title(window_title)\n",
    "    \n",
    "    # write classes in y axis\n",
    "    tick_font_size = 12\n",
    "    plt.yticks(range(n_classes), sorted_keys, fontsize=tick_font_size)\n",
    "    \"\"\"\n",
    "     Re-scale height accordingly\n",
    "    \"\"\"\n",
    "    init_height = fig.get_figheight()\n",
    "    # comput the matrix height in points and inches\n",
    "    dpi = fig.dpi\n",
    "    height_pt = n_classes * (tick_font_size * 1.4) # 1.4 (some spacing)\n",
    "    height_in = height_pt / dpi\n",
    "    # compute the required figure height\n",
    "    top_margin = 0.15    # in percentage of the figure height\n",
    "    bottom_margin = 0.05 # in percentage of the figure height\n",
    "    figure_height = height_in / (1 - top_margin - bottom_margin)\n",
    "    # set new height\n",
    "    if figure_height > init_height:\n",
    "        fig.set_figheight(figure_height)\n",
    "\n",
    "    # set plot title\n",
    "    plt.title(plot_title, fontsize=14)\n",
    "    # set axis titles\n",
    "    # plt.xlabel('classes')\n",
    "    plt.xlabel(x_label, fontsize='large')\n",
    "    # adjust size of window\n",
    "    fig.tight_layout()\n",
    "    # save the plot\n",
    "    fig.savefig(output_path)\n",
    "    # show image\n",
    "    if to_show:\n",
    "        plt.show()\n",
    "    # close the plot\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "450b4c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_AP(gt_records, pred_records, class_name, iou_threshold, show_result):\n",
    "    '''\n",
    "    Calculate AP value for one class records\n",
    "\n",
    "    Param\n",
    "         gt_records: ground truth records list for one class, with format:\n",
    "                     [\n",
    "                      ['image_file', 'xmin,ymin,xmax,ymax'],\n",
    "                      ['image_file', 'xmin,ymin,xmax,ymax'],\n",
    "                      ...\n",
    "                     ]\n",
    "         pred_records: predict records for one class, with format (in score descending order):\n",
    "                     [\n",
    "                      ['image_file', 'xmin,ymin,xmax,ymax', score],\n",
    "                      ['image_file', 'xmin,ymin,xmax,ymax', score],\n",
    "                      ...\n",
    "                     ]\n",
    "    Return\n",
    "         AP value for the class\n",
    "    '''\n",
    "    # append usage flag in gt_records for matching gt search\n",
    "    gt_records = [gt_record + ['unused'] for gt_record in gt_records]\n",
    "\n",
    "    # prepare score list for generating P-R html page\n",
    "    scores = [pred_record[2] for pred_record in pred_records]\n",
    "\n",
    "    # init true_positive and false_positive list\n",
    "    nd = len(pred_records)  # number of predict data\n",
    "    true_positive = [0] * nd\n",
    "    false_positive = [0] * nd\n",
    "    true_positive_count = 0\n",
    "    # assign predictions to ground truth objects\n",
    "    for idx, pred_record in enumerate(pred_records):\n",
    "        # filter out gt record from same image\n",
    "        image_gt_records = [ gt_record for gt_record in gt_records if gt_record[0] == pred_record[0]]\n",
    "\n",
    "        i = match_gt_box(pred_record, image_gt_records, iou_threshold=iou_threshold)\n",
    "        if i != -1:\n",
    "            # find a valid gt obj to assign, set\n",
    "            # true_positive list and mark image_gt_records.\n",
    "            #\n",
    "            # trick: gt_records will also be marked\n",
    "            # as 'used', since image_gt_records is a\n",
    "            # reference list\n",
    "            image_gt_records[i][2] = 'used'\n",
    "            true_positive[idx] = 1\n",
    "            true_positive_count += 1\n",
    "        else:\n",
    "            false_positive[idx] = 1\n",
    "\n",
    "    # compute precision/recall\n",
    "    rec, prec = get_rec_prec(true_positive, false_positive, gt_records)\n",
    "    ap, mrec, mprec = voc_ap(rec, prec)\n",
    "    if show_result:\n",
    "        #draw_rec_prec(rec, prec, mrec, mprec, class_name, ap)\n",
    "        generate_rec_prec_html(mrec, mprec, scores, class_name, ap)\n",
    "\n",
    "    return ap, true_positive_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42b816a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_Pascal_AP_result(count_images, count_true_positives, num_classes,\n",
    "                          gt_counter_per_class, pred_counter_per_class,\n",
    "                          precision_dict, recall_dict, mPrec, mRec,\n",
    "                          APs, mAP, iou_threshold):\n",
    "    '''\n",
    "     Plot the total number of occurences of each class in the ground-truth\n",
    "    '''\n",
    "    window_title = \"Ground-Truth Info\"\n",
    "    plot_title = \"Ground-Truth\\n\" + \"(\" + str(count_images) + \" files and \" + str(num_classes) + \" classes)\"\n",
    "    x_label = \"Number of objects per class\"\n",
    "    output_path = os.path.join(_dir+'/result/Ground-Truth_Info.png')\n",
    "    draw_plot_func(gt_counter_per_class, num_classes, window_title, plot_title, x_label, output_path, to_show=False, plot_color='forestgreen', true_p_bar='')\n",
    "\n",
    "    '''\n",
    "     Plot the total number of occurences of each class in the \"predicted\" folder\n",
    "    '''\n",
    "    window_title = \"Predicted Objects Info\"\n",
    "    # Plot title\n",
    "    plot_title = \"Predicted Objects\\n\" + \"(\" + str(count_images) + \" files and \"\n",
    "    count_non_zero_values_in_dictionary = sum(int(x) > 0 for x in list(pred_counter_per_class.values()))\n",
    "    plot_title += str(count_non_zero_values_in_dictionary) + \" detected classes)\"\n",
    "    # end Plot title\n",
    "    x_label = \"Number of objects per class\"\n",
    "    output_path = os.path.join(_dir+'/result/Predicted_Objects_Info.png')\n",
    "    draw_plot_func(pred_counter_per_class, len(pred_counter_per_class), window_title, plot_title, x_label, output_path, to_show=False, plot_color='forestgreen', true_p_bar=count_true_positives)\n",
    "\n",
    "    '''\n",
    "     Draw mAP plot (Show AP's of all classes in decreasing order)\n",
    "    '''\n",
    "    window_title = \"mAP\"\n",
    "    plot_title = \"mAP@IoU={0}: {1:.2f}%\".format(iou_threshold, mAP)\n",
    "    x_label = \"Average Precision\"\n",
    "    output_path = os.path.join(_dir+'/result/mAP.png')\n",
    "    draw_plot_func(APs, num_classes, window_title, plot_title, x_label, output_path, to_show=False, plot_color='royalblue', true_p_bar='')\n",
    "\n",
    "    '''\n",
    "     Draw Precision plot (Show Precision of all classes in decreasing order)\n",
    "    '''\n",
    "    window_title = \"Precision\"\n",
    "    plot_title = \"mPrec@IoU={0}: {1:.2f}%\".format(iou_threshold, mPrec)\n",
    "    x_label = \"Precision rate\"\n",
    "    output_path = os.path.join(_dir+'/result/Precision.png')\n",
    "    draw_plot_func(precision_dict, len(precision_dict), window_title, plot_title, x_label, output_path, to_show=False, plot_color='royalblue', true_p_bar='')\n",
    "\n",
    "    '''\n",
    "     Draw Recall plot (Show Recall of all classes in decreasing order)\n",
    "    '''\n",
    "    window_title = \"Recall\"\n",
    "    plot_title = \"mRec@IoU={0}: {1:.2f}%\".format(iou_threshold, mRec)\n",
    "    x_label = \"Recall rate\"\n",
    "    output_path = os.path.join(_dir+'/result/Recall.png')\n",
    "    draw_plot_func(recall_dict, len(recall_dict), window_title, plot_title, x_label, output_path, to_show=False, plot_color='royalblue', true_p_bar='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6db7950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_metric(metric_records, gt_classes_records):\n",
    "    '''\n",
    "    Calculate mean metric, but only count classes which have ground truth object\n",
    "\n",
    "    Param\n",
    "        metric_records: metric dict like:\n",
    "            metric_records = {\n",
    "                'aeroplane': 0.79,\n",
    "                'bicycle': 0.79,\n",
    "                    ...\n",
    "                'tvmonitor': 0.71,\n",
    "            }\n",
    "        gt_classes_records: ground truth class dict like:\n",
    "            gt_classes_records = {\n",
    "                'car': [\n",
    "                    ['000001.jpg','100,120,200,235'],\n",
    "                    ['000002.jpg','85,63,156,128'],\n",
    "                    ...\n",
    "                    ],\n",
    "                ...\n",
    "            }\n",
    "    Return\n",
    "         mean_metric: float value of mean metric\n",
    "    '''\n",
    "    mean_metric = 0.0\n",
    "    count = 0\n",
    "    for (class_name, metric) in metric_records.items():\n",
    "        if (class_name in gt_classes_records) and (len(gt_classes_records[class_name]) != 0):\n",
    "            mean_metric += metric\n",
    "            count += 1\n",
    "    mean_metric = (mean_metric/count)*100 if count != 0 else 0.0\n",
    "    return mean_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "155a6a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mAP_PascalVOC(annotation_records, gt_classes_records, pred_classes_records, class_names, iou_threshold, show_result=True):\n",
    "    '''\n",
    "    Compute PascalVOC style mAP\n",
    "    '''\n",
    "    APs = {}\n",
    "    count_true_positives = {class_name: 0 for class_name in list(gt_classes_records.keys())}\n",
    "    #print(pred_classes_records)\n",
    "    #get AP value for each of the ground truth classes\n",
    "    s=time.time()\n",
    "    for _, class_name in enumerate(class_names):\n",
    "        #if there's no gt obj for a class, record 0\n",
    "        if class_name not in gt_classes_records:\n",
    "            APs[class_name] = 0.\n",
    "            continue\n",
    "        gt_records = gt_classes_records[class_name]\n",
    "        #if we didn't detect any obj for a class, record 0\n",
    "        if class_name not in pred_classes_records:\n",
    "            APs[class_name] = 0.\n",
    "            continue\n",
    "        pred_records = pred_classes_records[class_name]\n",
    "        ap, true_positive_count = calc_AP(gt_records, pred_records, class_name, iou_threshold, show_result)\n",
    "        APs[class_name] = ap\n",
    "        count_true_positives[class_name] = true_positive_count\n",
    "    e=time.time()\n",
    "    \n",
    "    ss=time.time()\n",
    "    #sort AP result by value, in descending order\n",
    "    APs = OrderedDict(sorted(APs.items(), key=operator.itemgetter(1), reverse=True))\n",
    "\n",
    "    #get mAP percentage value\n",
    "    #mAP = np.mean(list(APs.values()))*100\n",
    "    mAP = get_mean_metric(APs, gt_classes_records)\n",
    "\n",
    "    #get GroundTruth count per class\n",
    "    gt_counter_per_class = {}\n",
    "    for (class_name, info_list) in gt_classes_records.items():\n",
    "        gt_counter_per_class[class_name] = len(info_list)\n",
    "\n",
    "    #get Precision count per class\n",
    "    pred_counter_per_class = {class_name: 0 for class_name in list(gt_classes_records.keys())}\n",
    "    for (class_name, info_list) in pred_classes_records.items():\n",
    "        pred_counter_per_class[class_name] = len(info_list)\n",
    "\n",
    "\n",
    "    #get the precision & recall\n",
    "    precision_dict = {}\n",
    "    recall_dict = {}\n",
    "    for (class_name, gt_count) in gt_counter_per_class.items():\n",
    "        if (class_name not in pred_counter_per_class) or (class_name not in count_true_positives) or pred_counter_per_class[class_name] == 0:\n",
    "            precision_dict[class_name] = 0.\n",
    "        else:\n",
    "            precision_dict[class_name] = float(count_true_positives[class_name]) / pred_counter_per_class[class_name]\n",
    "\n",
    "        if class_name not in count_true_positives or gt_count == 0:\n",
    "            recall_dict[class_name] = 0.\n",
    "        else:\n",
    "            recall_dict[class_name] = float(count_true_positives[class_name]) / gt_count\n",
    "\n",
    "    #get mPrec, mRec\n",
    "    #mPrec = np.mean(list(precision_dict.values()))*100\n",
    "    #mRec = np.mean(list(recall_dict.values()))*100\n",
    "    mPrec = get_mean_metric(precision_dict, gt_classes_records)\n",
    "    mRec = get_mean_metric(recall_dict, gt_classes_records)\n",
    "    ee=time.time()\n",
    "    \n",
    "    print(f'times are : {e-s},{ee-ss}')\n",
    "\n",
    "    if show_result:\n",
    "        plot_Pascal_AP_result(len(annotation_records), count_true_positives, len(gt_classes_records),\n",
    "                                  gt_counter_per_class, pred_counter_per_class,\n",
    "                                  precision_dict, recall_dict, mPrec, mRec,\n",
    "                                  APs, mAP, iou_threshold)\n",
    "        #show result\n",
    "        print('\\nPascal VOC AP evaluation')\n",
    "        for (class_name, AP) in APs.items():\n",
    "            print('%s: AP %.4f, precision %.4f, recall %.4f' % (class_name, AP, precision_dict[class_name], recall_dict[class_name]))\n",
    "        print('mAP@IoU=%.2f result: %f' % (iou_threshold, mAP))\n",
    "        print('mPrec@IoU=%.2f result: %f' % (iou_threshold, mPrec))\n",
    "        print('mRec@IoU=%.2f result: %f' % (iou_threshold, mRec))\n",
    "\n",
    "    #return mAP percentage value\n",
    "    return mAP, APs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "023808e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mAP_PascalVOC_multithread(_class_names, iou_threshold, thread_indx=-1, show_result=True):\n",
    "    '''\n",
    "    Compute PascalVOC style mAP\n",
    "    '''\n",
    "    \n",
    "    global model_input_shape, anchors, annotation_lines, annotation_records, gt_classes_records, all_pred_classes_records, APs, count_true_positives\n",
    "    #print(pred_classes_records)\n",
    "    #get AP value for each of the ground truth classes\n",
    "    print(f'Thread {thread_indx} computing mAP for {_class_names}')\n",
    "    start_time=time.time()\n",
    "    pred_classes_records=OrderedDict()\n",
    "    for _, class_name in enumerate(_class_names):\n",
    "        for predicts in all_pred_classes_records:\n",
    "            if class_name in predicts:\n",
    "                if class_name in pred_classes_records:\n",
    "                    pred_classes_records[class_name] += predicts[class_name].copy()      \n",
    "                else:\n",
    "                    pred_classes_records[class_name] = predicts[class_name].copy()\n",
    "    #return pred_classes_records\n",
    "        pred_classes_records[class_name]=sorted(pred_classes_records[class_name], key=lambda x:-x[-1])\n",
    "        #if there's no gt obj for a class, record 0\n",
    "        if class_name not in gt_classes_records:\n",
    "            APs[class_name] = 0.\n",
    "            continue\n",
    "        gt_records = gt_classes_records[class_name]\n",
    "        #if we didn't detect any obj for a class, record 0\n",
    "        if class_name not in pred_classes_records:\n",
    "            APs[class_name] = 0.\n",
    "            continue\n",
    "        pred_records = pred_classes_records[class_name]\n",
    "        ap, true_positive_count = calc_AP(gt_records, pred_records, class_name, iou_threshold, show_result)\n",
    "        APs[class_name] = ap\n",
    "        count_true_positives[class_name] = true_positive_count\n",
    "    end_time=time.time()\n",
    "    print(f'Compute AP for thread {thread_indx} time: {end_time-start_time}')\n",
    "    return \n",
    "\n",
    "def reduce(iou_threshold, show_result=True):\n",
    "    global class_names, anchors, annotation_lines, annotation_records, gt_classes_records, all_pred_classes_records, APs, count_true_positives\n",
    "    ss=time.time()\n",
    "    #sort AP result by value, in descending order\n",
    "    APs = OrderedDict(sorted(APs.items(), key=operator.itemgetter(1), reverse=True))\n",
    "\n",
    "    #get mAP percentage value\n",
    "    #mAP = np.mean(list(APs.values()))*100\n",
    "    mAP = get_mean_metric(APs, gt_classes_records)\n",
    "\n",
    "    #get GroundTruth count per class\n",
    "    gt_counter_per_class = {}\n",
    "    for (class_name, info_list) in gt_classes_records.items():\n",
    "        gt_counter_per_class[class_name] = len(info_list)\n",
    "\n",
    "    #get Precision count per class\n",
    "    pred_counter_per_class = {class_name: 0 for class_name in list(gt_classes_records.keys())}\n",
    "    for pred_classes_records in all_pred_classes_records:\n",
    "        for (class_name, info_list) in pred_classes_records.items():\n",
    "            pred_counter_per_class[class_name] += len(info_list)\n",
    "\n",
    "\n",
    "    #get the precision & recall\n",
    "    precision_dict = {}\n",
    "    recall_dict = {}\n",
    "    for (class_name, gt_count) in gt_counter_per_class.items():\n",
    "        if (class_name not in pred_counter_per_class) or (class_name not in count_true_positives) or pred_counter_per_class[class_name] == 0:\n",
    "            precision_dict[class_name] = 0.\n",
    "        else:\n",
    "            precision_dict[class_name] = float(count_true_positives[class_name]) / pred_counter_per_class[class_name]\n",
    "\n",
    "        if class_name not in count_true_positives or gt_count == 0:\n",
    "            recall_dict[class_name] = 0.\n",
    "        else:\n",
    "            recall_dict[class_name] = float(count_true_positives[class_name]) / gt_count\n",
    "\n",
    "    #get mPrec, mRec\n",
    "    #mPrec = np.mean(list(precision_dict.values()))*100\n",
    "    #mRec = np.mean(list(recall_dict.values()))*100\n",
    "    mPrec = get_mean_metric(precision_dict, gt_classes_records)\n",
    "    mRec = get_mean_metric(recall_dict, gt_classes_records)\n",
    "    \n",
    "\n",
    "    if show_result:\n",
    "        '''plot_Pascal_AP_result(len(annotation_records), count_true_positives, len(gt_classes_records),\n",
    "                                  gt_counter_per_class, pred_counter_per_class,\n",
    "                                  precision_dict, recall_dict, mPrec, mRec,\n",
    "                                  APs, mAP, iou_threshold)'''\n",
    "        #show result\n",
    "        print('\\nPascal VOC AP evaluation')\n",
    "        #for (class_name, AP) in APs.items():\n",
    "        #    print('%s: AP %.4f, precision %.4f, recall %.4f' % (class_name, AP, precision_dict[class_name], recall_dict[class_name]))\n",
    "        print('mAP@IoU=%.2f result: %f' % (iou_threshold, mAP))\n",
    "        print('mPrec@IoU=%.2f result: %f' % (iou_threshold, mPrec))\n",
    "        print('mRec@IoU=%.2f result: %f' % (iou_threshold, mRec))\n",
    "\n",
    "    ee=time.time()\n",
    "    print(f'reduc time : {ee-ss}')\n",
    "    #return mAP percentage value\n",
    "    return mAP, APs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c08bcf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_AP_COCO(annotation_records, gt_classes_records, pred_classes_records, class_names, class_filter=None, show_result=True):\n",
    "    '''\n",
    "    Compute MSCOCO AP list on AP 0.5:0.05:0.95\n",
    "    '''\n",
    "    iou_threshold_list = [x/100 for x in range(50, 100, 5)]\n",
    "    APs = {}\n",
    "    pbar = tqdm(total=len(iou_threshold_list), desc='Eval COCO')\n",
    "    for iou_threshold in iou_threshold_list:\n",
    "        iou_threshold = round(iou_threshold, 2)\n",
    "        mAP, mAPs = compute_mAP_PascalVOC(annotation_records, gt_classes_records, pred_classes_records, class_names, iou_threshold, show_result=False)\n",
    "\n",
    "        if class_filter is not None:\n",
    "            mAP = get_filter_class_mAP(mAPs, class_filter, show_result=False)\n",
    "\n",
    "        APs[iou_threshold] = round(mAP, 6)\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    #sort AP result by value, in descending order\n",
    "    APs = OrderedDict(sorted(APs.items(), key=operator.itemgetter(1), reverse=True))\n",
    "\n",
    "    #get overall AP percentage value\n",
    "    AP = np.mean(list(APs.values()))\n",
    "\n",
    "    if show_result:\n",
    "        '''\n",
    "         Draw MS COCO AP plot\n",
    "        '''\n",
    "        os.makedirs(_dir+'/result', exist_ok=True)\n",
    "        window_title = \"MSCOCO AP on different IOU\"\n",
    "        plot_title = \"COCO AP = {0:.2f}%\".format(AP)\n",
    "        x_label = \"Average Precision\"\n",
    "        output_path = os.path.join(_dir+'/result/COCO_AP.png')\n",
    "        draw_plot_func(APs, len(APs), window_title, plot_title, x_label, output_path, to_show=False, plot_color='royalblue', true_p_bar='')\n",
    "\n",
    "        print('\\nMS COCO AP evaluation')\n",
    "        for (iou_threshold, AP_value) in APs.items():\n",
    "            print('IOU %.2f: AP %f' % (iou_threshold, AP_value))\n",
    "        print('total AP: %f' % (AP))\n",
    "\n",
    "    #return AP percentage value\n",
    "    return AP, APs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "388a493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_AP_COCO_Scale(annotation_records, scale_gt_classes_records, pred_classes_records, class_names):\n",
    "    '''\n",
    "    Compute MSCOCO AP on different scale object: small, medium, large\n",
    "    '''\n",
    "    scale_APs = {}\n",
    "    for scale_key in ['small','medium','large']:\n",
    "        gt_classes_records = scale_gt_classes_records[scale_key]\n",
    "        scale_AP, _ = compute_AP_COCO(annotation_records, gt_classes_records, pred_classes_records, class_names, show_result=False)\n",
    "        scale_APs[scale_key] = round(scale_AP, 4)\n",
    "\n",
    "    #get overall AP percentage value\n",
    "    scale_mAP = np.mean(list(scale_APs.values()))\n",
    "\n",
    "    '''\n",
    "     Draw Scale AP plot\n",
    "    '''\n",
    "    os.makedirs(_dir+'/result', exist_ok=True)\n",
    "    window_title = \"MSCOCO AP on different scale\"\n",
    "    plot_title = \"scale mAP = {0:.2f}%\".format(scale_mAP)\n",
    "    x_label = \"Average Precision\"\n",
    "    output_path = os.path.join(_dir+'/result/COCO_scale_AP.png')\n",
    "    draw_plot_func(scale_APs, len(scale_APs), window_title, plot_title, x_label, output_path, to_show=False, plot_color='royalblue', true_p_bar='')\n",
    "\n",
    "    '''\n",
    "     Draw Scale Object Sum plot\n",
    "    '''\n",
    "    for scale_key in ['small','medium','large']:\n",
    "        gt_classes_records = scale_gt_classes_records[scale_key]\n",
    "        gt_classes_sum = {}\n",
    "\n",
    "        for _, class_name in enumerate(class_names):\n",
    "            # summarize the gt object number for every class on different scale\n",
    "            gt_classes_sum[class_name] = np.sum(len(gt_classes_records[class_name])) if class_name in gt_classes_records else 0\n",
    "\n",
    "        total_sum = np.sum(list(gt_classes_sum.values()))\n",
    "\n",
    "        window_title = \"{} object number\".format(scale_key)\n",
    "        plot_title = \"total {} object number = {}\".format(scale_key, total_sum)\n",
    "        x_label = \"Object Number\"\n",
    "        output_path = os.path.join(_dir+'/result','{}_object_number.png'.format(scale_key))\n",
    "        draw_plot_func(gt_classes_sum, len(gt_classes_sum), window_title, plot_title, x_label, output_path, to_show=False, plot_color='royalblue', true_p_bar='')\n",
    "\n",
    "    print('\\nMS COCO AP evaluation on different scale')\n",
    "    for (scale, AP_value) in scale_APs.items():\n",
    "        print('%s scale: AP %f' % (scale, AP_value))\n",
    "    print('total AP: %f' % (scale_mAP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c1c985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gt_record(gt_records, gt_record, class_name):\n",
    "    # append or add ground truth class item\n",
    "    if class_name in gt_records:\n",
    "        gt_records[class_name].append(gt_record)\n",
    "    else:\n",
    "        gt_records[class_name] = list([gt_record])\n",
    "\n",
    "    return gt_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "704575ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scale_gt_dict(gt_classes_records, class_names):\n",
    "    '''\n",
    "    Get ground truth class dict on different object scales, according to MS COCO metrics definition:\n",
    "        small objects: area < 32^2\n",
    "        medium objects: 32^2 < area < 96^2\n",
    "        large objects: area > 96^2\n",
    "\n",
    "    input gt_classes_records would be like:\n",
    "    gt_classes_records = {\n",
    "        'car': [\n",
    "                ['000001.jpg','100,120,200,235'],\n",
    "                ['000002.jpg','85,63,156,128'],\n",
    "                ...\n",
    "               ],\n",
    "        ...\n",
    "    }\n",
    "    return a record dict with following format, for AP/AR eval on different scale:\n",
    "        scale_gt_classes_records = {\n",
    "            'small': {\n",
    "                'car': [\n",
    "                        ['000001.jpg','100,120,200,235'],\n",
    "                        ['000002.jpg','85,63,156,128'],\n",
    "                        ...\n",
    "                       ],\n",
    "                ...\n",
    "            },\n",
    "\n",
    "            'medium': {\n",
    "                'car': [\n",
    "                        ['000003.jpg','100,120,200,235'],\n",
    "                        ['000004.jpg','85,63,156,128'],\n",
    "                        ...\n",
    "                       ],\n",
    "                ...\n",
    "            },\n",
    "\n",
    "            'large': {\n",
    "                'car': [\n",
    "                        ['000005.jpg','100,120,200,235'],\n",
    "                        ['000006.jpg','85,63,156,128'],\n",
    "                        ...\n",
    "                       ],\n",
    "                ...\n",
    "            }\n",
    "        }\n",
    "    '''\n",
    "    scale_gt_classes_records = {}\n",
    "    small_gt_records = {}\n",
    "    medium_gt_records = {}\n",
    "    large_gt_records = {}\n",
    "\n",
    "    for _, class_name in enumerate(class_names):\n",
    "        gt_records = gt_classes_records[class_name]\n",
    "\n",
    "        for (image_file, box) in gt_records:\n",
    "            # get box area based on coordinate\n",
    "            box_coord = [int(p) for p in box.split(',')]\n",
    "            box_area = (box_coord[2] - box_coord[0]) * (box_coord[3] - box_coord[1])\n",
    "\n",
    "            # add to corresponding gt records dict according to area size\n",
    "            if box_area <= 32*32:\n",
    "                small_gt_records = add_gt_record(small_gt_records, [image_file, box], class_name)\n",
    "            elif box_area > 32*32 and box_area <= 96*96:\n",
    "                medium_gt_records = add_gt_record(medium_gt_records, [image_file, box], class_name)\n",
    "            elif box_area > 96*96:\n",
    "                large_gt_records = add_gt_record(large_gt_records, [image_file, box], class_name)\n",
    "\n",
    "    # form up scale_gt_classes_records\n",
    "    scale_gt_classes_records['small'] = small_gt_records\n",
    "    scale_gt_classes_records['medium'] = medium_gt_records\n",
    "    scale_gt_classes_records['large'] = large_gt_records\n",
    "\n",
    "    return scale_gt_classes_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b041614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filter_class_mAP(APs, class_filter, show_result=True):\n",
    "    filtered_mAP = 0.0\n",
    "    filtered_APs = OrderedDict()\n",
    "\n",
    "    for (class_name, AP) in APs.items():\n",
    "        if class_name in class_filter:\n",
    "            filtered_APs[class_name] = AP\n",
    "\n",
    "    filtered_mAP = np.mean(list(filtered_APs.values()))*100\n",
    "\n",
    "    if show_result:\n",
    "        print('\\nfiltered classes AP')\n",
    "        for (class_name, AP) in filtered_APs.items():\n",
    "            print('%s: AP %.4f' % (class_name, AP))\n",
    "        print('mAP:', filtered_mAP, '\\n')\n",
    "    return filtered_mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ccb4015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_AP_multithread(model_path, eval_type, iou_threshold, conf_threshold, elim_grid_sense, v5_decode, save_result, class_filter=None, start_indx=-1, end_indx=-1, thread_indx=-1):\n",
    "    '''                \n",
    "    Compute AP for detection model on annotation dataset\n",
    "    '''\n",
    "    \n",
    "    global class_names, model_input_shape, all_pred_classes_records, anchors\n",
    "    start = time.time()\n",
    "    model, model_format = load_eval_model(model_path, model_input_shape)\n",
    "    end = time.time()\n",
    "    if thread_indx==0:\n",
    "        print(\"load eval model time cost: {:.6f}s\".format(end - start))\n",
    "    \n",
    "    \n",
    "    item_slice = annotation_records\n",
    "    if start_indx != -1:\n",
    "        item_slice = itertools.islice(annotation_records.items(), start_indx, end_indx)\n",
    "        item_slice=collections.OrderedDict(item_slice)\n",
    "        num_items = sum(1 for _ in item_slice.items())\n",
    "        if thread_indx==0:\n",
    "            print(f'number of images per thread: {num_items}')\n",
    "        #for k,i in item_slice.items():\n",
    "        #    print(k,i)\n",
    "    #Ehsan\n",
    "    caching=False\n",
    "    res_name=\"test_\"+str(thread_indx)+\".pkl\"\n",
    "    \n",
    "    '''if os.path.isfile(res_name) and caching:\n",
    "        with open(res_name,'rb') as f:\n",
    "            pred_classes_records=pickle.load(f)\n",
    "            all_pred_classes_records[thread_indx]=pred_classes_records.copy()'''\n",
    "    #else:\n",
    "    pred_classes_records = get_prediction_class_records(model, model_format, item_slice, anchors, class_names, model_input_shape, conf_threshold, elim_grid_sense, v5_decode, save_result,indx=thread_indx)\n",
    "    all_pred_classes_records[thread_indx]=pred_classes_records.copy()\n",
    "    end = time.time()\n",
    "    print(\"Inference time for thread {} cost: {:.6f}s\".format(thread_indx, end - start))\n",
    "    #with open(res_name,'wb') as f:\n",
    "    #    pickle.dump(pred_classes_records,f)\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "418fc2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load TF 1.x frozen pb graph\n",
    "def load_graph(model_path):\n",
    "    # check tf version to be compatible with TF 2.x\n",
    "    global tf\n",
    "    if tf.__version__.startswith('2'):\n",
    "        import tensorflow.compat.v1 as tf\n",
    "        tf.disable_eager_execution()\n",
    "\n",
    "    # We parse the graph_def file\n",
    "    with tf.gfile.GFile(model_path, \"rb\") as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "\n",
    "    # We load the graph_def in the default graph\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(\n",
    "            graph_def,\n",
    "            input_map=None,\n",
    "            return_elements=None,\n",
    "            name=\"graph\",\n",
    "            op_dict=None,\n",
    "            producer_op_list=None\n",
    "        )\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ef80c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eval_model(model_path,model_input_shape):\n",
    "    \n",
    "    # support of tflite model\n",
    "    if model_path.endswith('.tflite'):\n",
    "        from tensorflow.lite.python import interpreter as interpreter_wrapper\n",
    "        model = interpreter_wrapper.Interpreter(model_path=model_path)\n",
    "        model.allocate_tensors()\n",
    "        model_format = 'TFLITE'\n",
    "\n",
    "        #Ehsan input shape correctness\n",
    "        input_details = model.get_input_details()\n",
    "        input_shape = input_details[0]['shape']\n",
    "        input_shape[1] = model_input_shape[0]\n",
    "        input_shape[2] = model_input_shape[1]\n",
    "        model.resize_tensor_input(0, input_shape)\n",
    "        model.allocate_tensors()\n",
    "\n",
    "\n",
    "    # support of MNN model\n",
    "    elif model_path.endswith('.mnn'):\n",
    "        model = MNN.Interpreter(model_path)\n",
    "        model_format = 'MNN'\n",
    "\n",
    "    # support of TF 1.x frozen pb model\n",
    "    elif model_path.endswith('.pb'):\n",
    "        model = load_graph(model_path)\n",
    "        model_format = 'PB'\n",
    "\n",
    "    # support of ONNX model\n",
    "    elif model_path.endswith('.onnx'):\n",
    "        model = onnxruntime.InferenceSession(model_path)\n",
    "        model_format = 'ONNX'\n",
    "\n",
    "    # normal keras h5 model\n",
    "    elif model_path.endswith('.h5'):\n",
    "        custom_object_dict = get_custom_objects()\n",
    "\n",
    "        model = load_model(model_path, compile=False, custom_objects=custom_object_dict)\n",
    "        model_format = 'H5'\n",
    "        K.set_learning_phase(0)\n",
    "    else:\n",
    "        raise ValueError('invalid model file')\n",
    "\n",
    "    return model, model_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03276dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(raw_args=None):\n",
    "    # class YOLO defines the default value, so suppress any default here\n",
    "    #parser = argparse.ArgumentParser(argument_default=argparse.SUPPRESS, description='evaluate YOLO model (h5/pb/onnx/tflite/mnn) with test dataset')\n",
    "    global model_input_shape, class_names, class_filter, anchors, annotation_lines, annotation_records, gt_classes_records, all_pred_classes_records, APs, count_true_positives\n",
    "    start_time=time.time()\n",
    "    parser = argparse.ArgumentParser(sys.argv)\n",
    "    os.makedirs(_dir+'/Predicts', exist_ok=True)\n",
    "    '''\n",
    "    Command line options\n",
    "    '''\n",
    "    parser.add_argument(\n",
    "        '--model_path', type=str, required=False,\n",
    "        help='path to model file',default=os.path.join(_dir,'Models/Yolov3.h5'))\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--anchors_path', type=str, required=False,\n",
    "        help='path to anchor definitions',default=os.path.join(_dir,\"configs/yolo3_anchors.txt\"))\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--classes_path', type=str, required=False,\n",
    "        help='path to class definitions, default=%(default)s', default=os.path.join(_dir , 'configs/coco_classes.txt'))\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--classes_filter_path', type=str, required=False,\n",
    "        help='path to class filter definitions, default=%(default)s', default=None)\n",
    "\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--annotation_file', type=str, required=False,\n",
    "        help='test annotation txt file',default=os.path.join(_dir,'tools/dataset_converter/'+ann))\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--eval_type', type=str, required=True, choices=['VOC', 'COCO'],\n",
    "        help='evaluation type (VOC/COCO), default=%(default)s', default='COCO')\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--iou_threshold', type=float,\n",
    "        help='IOU threshold for PascalVOC mAP, default=%(default)s', default=0.5)\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--conf_threshold', type=float,\n",
    "        help='confidence threshold for filtering box in postprocess, default=%(default)s', default=0.001)\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--model_input_shape', type=str,\n",
    "        help='model image input shape as <height>x<width>, default=%(default)s', default='608x608')\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--elim_grid_sense', default=False, action=\"store_true\",\n",
    "        help = \"Eliminate grid sensitivity\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--v5_decode', default=False, action=\"store_true\",\n",
    "        help = \"Use YOLOv5 prediction decode\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--save_result', default=False, action=\"store_true\",\n",
    "        help='Save the detection result image in result/detection dir'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--num_threads', type=int,\n",
    "        help='number of threads for running evaluation, default=%(default)s', default=64)\n",
    "    parser.add_argument(\n",
    "        '--thread_indx', type=int,\n",
    "        help='Thread index, default=%(default)s', default=0)\n",
    "    parser.add_argument(\n",
    "        '--pkl_name', type=str,\n",
    "        help='pkl name for saving predicts, default=%(default)s', default=os.path.join(_dir+'/Predicts/','tmp.pkl'))\n",
    "\n",
    "    #_r=raw_args\n",
    "    #if len(sys.argv) > 10:\n",
    "    #    _r=sys.argv[1:]\n",
    "    print(f'cmd arguments are:{raw_args}')\n",
    "    args = parser.parse_args(raw_args+[\"--eval_type=VOC\"])\n",
    "    # param parse\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ################ Here global parameters are initialized ####################################\n",
    "    anchors = get_anchors(args.anchors_path)\n",
    "    class_names = get_classes(args.classes_path)\n",
    "    height, width = args.model_input_shape.split('x')\n",
    "    model_input_shape = (int(height), int(width))\n",
    "    assert (model_input_shape[0]%32 == 0 and model_input_shape[1]%32 == 0), 'model_input_shape should be multiples of 32'\n",
    "    # class filter parse\n",
    "    if args.classes_filter_path is not None:\n",
    "        class_filter = get_classes(args.classes_filter_path)\n",
    "    else:\n",
    "        class_filter = None\n",
    "    \n",
    "    all_pred_classes_records = [0] * args.num_threads    \n",
    "    start = time.time()\n",
    "    annotation_lines = get_dataset(args.annotation_file, shuffle=False)\n",
    "    APs = {}\n",
    "    annotation_records, gt_classes_records = annotation_parse(annotation_lines, class_names)\n",
    "    count_true_positives = {class_name: 0 for class_name in list(gt_classes_records.keys())}\n",
    "    end = time.time()\n",
    "    ############################################################################################\n",
    "    \n",
    "    end_time=time.time()\n",
    "    print(\"Initialization time cost: {:.6f}s\".format(end - start))\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7e16588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def merge_predicts(pkl_name):\n",
    "    global all_pred_classes_records\n",
    "    _pred_classes_records=all_pred_classes_records[0].copy()\n",
    "    for part in all_pred_classes_records[1:]:\n",
    "        for cls in part:\n",
    "            if cls in _pred_classes_records:\n",
    "                _pred_classes_records[cls] += part[cls].copy()\n",
    "            else:\n",
    "                _pred_classes_records[cls] = part[cls].copy()\n",
    "    del all_pred_classes_records[:]\n",
    "    all_pred_classes_records=_pred_classes_records\n",
    "    # Sort predicted images for each class based on last element in each predict list (which is probability)\n",
    "    for cls in all_pred_classes_records:\n",
    "        all_pred_classes_records[cls] = sorted(all_pred_classes_records[cls], key=lambda x: -x[-1])\n",
    "    \n",
    "    all_pred_classes_records=[all_pred_classes_records]\n",
    "    #os.makedirs(\"predicts/pkl/\", exist_ok=True)\n",
    "    with open(_dir+'/Predicts/'+pkl_name,'wb') as f:\n",
    "        pickle.dump(all_pred_classes_records,f)\n",
    "    APs={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc0b935e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"class_name='person'\\nprint(len(a[class_name]))\\nprint(len(b[class_name]))\\nprint(sum([len(k[class_name]) for k in D]))\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "def reload(p=\"Yolov3half_q.pkl\"):\n",
    "    global all_pred_classes_records\n",
    "    with open(p,'rb') as f:\n",
    "            all_pred_classes_records=[pickle.load(f)]\n",
    "    with open('Yolov3_s_32.pkl', 'rb') as f:\n",
    "            b=pickle.load(f)\n",
    "    #a['bicycle']\n",
    "\n",
    "def reload2():\n",
    "    n=64\n",
    "    global all_pred_classes_records\n",
    "    all_pred_classes_records=[]\n",
    "    for i in range(n):\n",
    "        with open('test_'+str(i)+'.pkl','rb') as f:\n",
    "            dd=pickle.load(f)\n",
    "            all_pred_classes_records.append(dd)\n",
    "\n",
    "'''class_name='person'\n",
    "print(len(a[class_name]))\n",
    "print(len(b[class_name]))\n",
    "print(sum([len(k[class_name]) for k in D]))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "32f37191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mltithread(args):\n",
    "    global class_names, class_filter, anchors, annotation_lines, annotation_records, gt_classes_records, all_pred_classes_records, APs, count_true_positives\n",
    "    N = len(annotation_lines) \n",
    "    num_threads = args.num_threads\n",
    "    print(f'number of threads: {num_threads}')\n",
    "    chunk_size = N // num_threads\n",
    "    threads = []\n",
    "    caching=True\n",
    "    start_time = time.time()\n",
    "    if os.path.isfile(_dir+'/Predicts/'+args.pkl_name) and caching:\n",
    "        with open(_dir+'/Predicts/'+args.pkl_name,'rb') as f:\n",
    "            all_pred_classes_records=pickle.load(f)\n",
    "    \n",
    "    \n",
    "    \n",
    "    else:\n",
    "        if num_threads > 0:\n",
    "\n",
    "            for i in range(num_threads):\n",
    "                start_i = i * chunk_size\n",
    "                end_i = start_i + chunk_size\n",
    "                if i == num_threads - 1:\n",
    "                    end_i= N\n",
    "                #item_slice = itertools.islice(data.items(), start, end)\n",
    "                thread = threading.Thread(target=eval_AP_multithread, args=(args.model_path, args.eval_type, args.iou_threshold, args.conf_threshold, args.elim_grid_sense, args.v5_decode, args.save_result, class_filter:=class_filter,start_indx:=start_i,end_indx:=end_i, thread_indx:=i))\n",
    "                threads.append(thread)\n",
    "                thread.start()\n",
    "            for thread in threads:\n",
    "                thread.join()\n",
    "\n",
    "            end_time = time.time()\n",
    "            print(\"Inference time: {:.6f}s\".format(end_time - start_time))\n",
    "        else:\n",
    "            eval_AP(model, model_format, annotation_lines, anchors, class_names, model_input_shape, args.eval_type, args.iou_threshold, args.conf_threshold, args.elim_grid_sense, args.v5_decode, args.save_result, class_filter=class_filter,start_indx=start_i,end_indx=end_i)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Inference time cost: {:.6f}s\".format(end_time - start_time))\n",
    "    \n",
    "    #### Merge predictions of all threads\n",
    "    merge_predicts(args.pkl_name)\n",
    "        \n",
    "    ###### Compute AP\n",
    "    num_threads=1\n",
    "    N = len(class_names) \n",
    "    num_threads = min(N,num_threads)\n",
    "    chunk_size = N // num_threads\n",
    "    threads = []\n",
    "    if num_threads > 0:\n",
    "        start_time = time.time()\n",
    "        for i in range(num_threads):\n",
    "            Y=[len(all_pred_classes_records[0][p]) for p in class_names]\n",
    "            Z = [x for _,x in sorted(zip(Y,class_names))]\n",
    "            sliced_classes=Z[i::num_threads]\n",
    "            '''start_i = i * chunk_size\n",
    "            end_i = start_i + chunk_size\n",
    "            if i == num_threads - 1:\n",
    "                end_i= N\n",
    "            sliced_classes=class_names[start_i:end_i]'''\n",
    "            thread = threading.Thread(target=compute_mAP_PascalVOC_multithread, args=(_class_names:=sliced_classes, iou_threshold:=args.iou_threshold, thread_indx:=i))\n",
    "            threads.append(thread)\n",
    "            thread.start()\n",
    "        \n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "        end_time = time.time()\n",
    "        print(\"Compute APs time: {:.6f}s\".format(end_time - start_time))\n",
    "    else:\n",
    "        start_time = time.time()\n",
    "        AP, APs = compute_mAP_PascalVOC(annotation_records, gt_classes_records, all_pred_classes_records, class_names, args.iou_threshold)\n",
    "        end_time=time.time()\n",
    "        print(\"Compute APs time: {:.6f}s\".format(end_time - start_time))\n",
    "        \n",
    "    ##### Reduce the computed APs for finall calculation\n",
    "    AP=0\n",
    "    mAP, APs=reduce(args.iou_threshold)\n",
    "    if class_filter is not None:\n",
    "        get_filter_class_mAP(APs, class_filter)\n",
    "    return mAP,APs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "740f7053",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "global class_names, class_filter, model_input_shape, anchors, annotation_lines, annotation_records, gt_classes_records, all_pred_classes_records, APs, count_true_positives\n",
    "def main(_args=[]):\n",
    "    start_time=time.time()\n",
    "    args=initialize(_args)\n",
    "    mAP,APs=run_mltithread(args)\n",
    "    end_time=time.time()\n",
    "    print(\"Total time: {:.6f}s\".format(end_time - start_time))\n",
    "    return mAP,APs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a96d9303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization time cost: 0.094646s\n",
      "number of threads: 32\n",
      "Inference time cost: 0.575567s\n",
      "Thread 0 computing mAP for ['hair drier', 'toaster', 'bear', 'microwave', 'snowboard', 'scissors', 'fire hydrant', 'parking meter', 'frisbee', 'aeroplane', 'toilet', 'toothbrush', 'cat', 'elephant', 'teddy bear', 'stop sign', 'mouse', 'train', 'hot dog', 'kite', 'keyboard', 'bed', 'baseball bat', 'giraffe', 'sandwich', 'refrigerator', 'dog', 'zebra', 'skateboard', 'baseball glove', 'tennis racket', 'laptop', 'bus', 'oven', 'horse', 'skis', 'pizza', 'orange', 'cow', 'sofa', 'motorbike', 'donut', 'suitcase', 'tvmonitor', 'sink', 'wine glass', 'fork', 'surfboard', 'cake', 'sheep', 'vase', 'truck', 'sports ball', 'clock', 'umbrella', 'tie', 'remote', 'bicycle', 'apple', 'boat', 'banana', 'backpack', 'broccoli', 'knife', 'spoon', 'carrot', 'bird', 'cell phone', 'bowl', 'pottedplant', 'traffic light', 'cup', 'bench', 'handbag', 'bottle', 'diningtable', 'book', 'car', 'chair', 'person']\n",
      "Compute AP for thread 0 time: 69.36982417106628\n",
      "Compute APs time: 69.381907s\n",
      "\n",
      "Pascal VOC AP evaluation\n",
      "cat: AP 0.9314, precision 0.3500, recall 0.9703\n",
      "fire hydrant: AP 0.9143, precision 0.2717, recall 0.9307\n",
      "frisbee: AP 0.9098, precision 0.2655, recall 0.9304\n",
      "train: AP 0.9095, precision 0.2812, recall 0.9368\n",
      "aeroplane: AP 0.8985, precision 0.3131, recall 0.9371\n",
      "bear: AP 0.8888, precision 0.4483, recall 0.9155\n",
      "giraffe: AP 0.8880, precision 0.3074, recall 0.9181\n",
      "horse: AP 0.8753, precision 0.2407, recall 0.9267\n",
      "tennis racket: AP 0.8721, precision 0.2270, recall 0.9111\n",
      "bus: AP 0.8663, precision 0.2615, recall 0.8982\n",
      "tvmonitor: AP 0.8623, precision 0.1996, recall 0.9410\n",
      "zebra: AP 0.8526, precision 0.2947, recall 0.9104\n",
      "dog: AP 0.8475, precision 0.2405, recall 0.9037\n",
      "mouse: AP 0.8388, precision 0.1566, recall 0.9057\n",
      "laptop: AP 0.8381, precision 0.2280, recall 0.8961\n",
      "stop sign: AP 0.8327, precision 0.1142, recall 0.9200\n",
      "clock: AP 0.8299, precision 0.1150, recall 0.8764\n",
      "skateboard: AP 0.8226, precision 0.1833, recall 0.8715\n",
      "parking meter: AP 0.8221, precision 0.1492, recall 0.9000\n",
      "refrigerator: AP 0.8189, precision 0.1465, recall 0.9048\n",
      "microwave: AP 0.8162, precision 0.2192, recall 0.8727\n",
      "toilet: AP 0.8143, precision 0.3521, recall 0.8715\n",
      "keyboard: AP 0.7969, precision 0.2165, recall 0.9085\n",
      "person: AP 0.7764, precision 0.1441, recall 0.9013\n",
      "teddy bear: AP 0.7518, precision 0.2721, recall 0.8534\n",
      "elephant: AP 0.7486, precision 0.3469, recall 0.8000\n",
      "surfboard: AP 0.7331, precision 0.1451, recall 0.8253\n",
      "bed: AP 0.7307, precision 0.2045, recall 0.8405\n",
      "motorbike: AP 0.7300, precision 0.2415, recall 0.8194\n",
      "baseball glove: AP 0.7199, precision 0.1356, recall 0.8108\n",
      "baseball bat: AP 0.7089, precision 0.1756, recall 0.8082\n",
      "sink: AP 0.7076, precision 0.1394, recall 0.8800\n",
      "cake: AP 0.6972, precision 0.1754, recall 0.8797\n",
      "cup: AP 0.6971, precision 0.1464, recall 0.8309\n",
      "car: AP 0.6964, precision 0.1149, recall 0.8406\n",
      "vase: AP 0.6932, precision 0.1419, recall 0.8448\n",
      "pizza: AP 0.6906, precision 0.2040, recall 0.8211\n",
      "sandwich: AP 0.6888, precision 0.1986, recall 0.8249\n",
      "sofa: AP 0.6886, precision 0.1716, recall 0.8161\n",
      "oven: AP 0.6846, precision 0.1261, recall 0.8741\n",
      "umbrella: AP 0.6767, precision 0.1647, recall 0.8257\n",
      "wine glass: AP 0.6692, precision 0.1853, recall 0.7988\n",
      "sports ball: AP 0.6668, precision 0.1208, recall 0.8023\n",
      "cell phone: AP 0.6474, precision 0.0583, recall 0.8092\n",
      "hot dog: AP 0.6468, precision 0.1552, recall 0.7795\n",
      "bowl: AP 0.6434, precision 0.1342, recall 0.8211\n",
      "bicycle: AP 0.6384, precision 0.1085, recall 0.7911\n",
      "bottle: AP 0.6365, precision 0.1286, recall 0.8215\n",
      "sheep: AP 0.6308, precision 0.1743, recall 0.7701\n",
      "pottedplant: AP 0.6290, precision 0.0729, recall 0.8484\n",
      "tie: AP 0.6162, precision 0.0927, recall 0.7598\n",
      "remote: AP 0.6076, precision 0.1012, recall 0.8127\n",
      "cow: AP 0.6036, precision 0.2272, recall 0.7421\n",
      "donut: AP 0.5890, precision 0.2280, recall 0.8728\n",
      "suitcase: AP 0.5834, precision 0.1821, recall 0.7921\n",
      "chair: AP 0.5834, precision 0.0964, recall 0.8040\n",
      "traffic light: AP 0.5783, precision 0.0945, recall 0.7363\n",
      "fork: AP 0.5766, precision 0.1077, recall 0.7581\n",
      "snowboard: AP 0.5757, precision 0.1589, recall 0.6957\n",
      "truck: AP 0.5644, precision 0.1749, recall 0.7108\n",
      "bird: AP 0.5365, precision 0.0877, recall 0.7159\n",
      "toothbrush: AP 0.4966, precision 0.0788, recall 0.7193\n",
      "diningtable: AP 0.4858, precision 0.0700, recall 0.7905\n",
      "scissors: AP 0.4823, precision 0.0655, recall 0.6111\n",
      "knife: AP 0.4781, precision 0.0718, recall 0.7147\n",
      "banana: AP 0.4768, precision 0.0930, recall 0.7388\n",
      "bench: AP 0.4750, precision 0.0502, recall 0.6780\n",
      "skis: AP 0.4706, precision 0.1426, recall 0.6432\n",
      "boat: AP 0.4687, precision 0.1102, recall 0.6791\n",
      "orange: AP 0.4659, precision 0.1653, recall 0.6934\n",
      "kite: AP 0.4447, precision 0.3094, recall 0.5893\n",
      "spoon: AP 0.4171, precision 0.0509, recall 0.6838\n",
      "backpack: AP 0.4077, precision 0.0810, recall 0.6819\n",
      "carrot: AP 0.3888, precision 0.0802, recall 0.7736\n",
      "toaster: AP 0.3806, precision 0.1333, recall 0.6667\n",
      "handbag: AP 0.3779, precision 0.0586, recall 0.6648\n",
      "broccoli: AP 0.3754, precision 0.0790, recall 0.7911\n",
      "apple: AP 0.3354, precision 0.0699, recall 0.7573\n",
      "book: AP 0.2624, precision 0.0829, recall 0.6589\n",
      "hair drier: AP 0.2500, precision 0.1154, recall 0.2727\n",
      "mAP@IoU=0.50 result: 65.912317\n",
      "mPrec@IoU=0.50 result: 16.784771\n",
      "mRec@IoU=0.50 result: 80.630689\n",
      "reduc time : 3.147162437438965\n",
      "Total time: 73.297274s\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(sys.argv[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "68c73b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    with open(\"test_0.pkl\",'rb') as f:\n",
    "        a=pickle.load(f)\n",
    "\n",
    "    with open(\"test_12.pkl\",'rb') as f:\n",
    "        b=pickle.load(f)\n",
    "\n",
    "    for i,j in a.items():\n",
    "        if i in b and i!='person':\n",
    "            print(\"a\")\n",
    "            print(i,b[i])\n",
    "            print(\"b\")\n",
    "            print(i,a[i])\n",
    "            break\n",
    "\n",
    "    a['bicycle']\n",
    "\n",
    "    b['bicycle']\n",
    "\n",
    "    for key, value in b.items():\n",
    "        if key in a:\n",
    "            a[key] += value\n",
    "        else:\n",
    "            a[key] = value\n",
    "\n",
    "    a['bicycle']"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "qe",
   "language": "python",
   "name": "qe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
